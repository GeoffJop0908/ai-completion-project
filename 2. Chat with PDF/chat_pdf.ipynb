{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "banlYBLgNyZy"
      },
      "source": [
        "# Chat with PDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WcYiq8f3OARY"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores.cassandra import Cassandra\n",
        "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
        "from langchain_openai import OpenAI\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "import cassio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "R-NTAdmNQ0j8"
      },
      "outputs": [],
      "source": [
        "from PyPDF2 import PdfReader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymdxTOEHRM-i"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvpJAdbRRN5D"
      },
      "source": [
        "## Providing the secrets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5KAdvK0uRPrY"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv()\n",
        "ASTRA_DB_APPLICATION_TOKEN = os.getenv(\"ASTRA_DB_APPLICATION_TOKEN\")\n",
        "ASTRA_DB_ID = os.getenv(\"ASTRA_DB_ID\")\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xFdE4lKmR5xL"
      },
      "outputs": [],
      "source": [
        "pdfreader = PdfReader('research_paper.pdf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "C3LkFac6SEM7"
      },
      "outputs": [],
      "source": [
        "raw_text = ''\n",
        "for i, page in enumerate(pdfreader.pages):\n",
        "  content = page.extract_text()\n",
        "  if content:\n",
        "    raw_text += content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-j9SyPpWSX6z",
        "outputId": "4c470b11-61d6-4926-b93e-e4b441f5a883"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10466 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 46, NO. 12, DECEMBER 2024\n",
            "A Survey on Graph Neural Networks for Time Series:\n",
            "Forecasting, Classiﬁcation, Imputation, and\n",
            "Anomaly Detection\n",
            "Ming Jin ,H u a nY e eK o h , Qingsong Wen , Daniele Zambon , Cesare Alippi , Fellow, IEEE ,\n",
            "Geoffrey I. Webb , Fellow, IEEE , Irwin King , Fellow, IEEE , and Shirui Pan , Senior Member, IEEE\n",
            "(Survey Paper)\n",
            "Abstract —Time series are the primary data type used to record\n",
            "dynamic system measurements and generated in great volume by\n",
            "both physical sensors and online processes (virtual sensors). Time\n",
            "series analytics is therefore crucial to unlocking the wealth of in-\n",
            "formation implicit in available data. With the recent advancements\n",
            "in graph neural networks (GNNs), there has been a surge in GNN-\n",
            "based approaches for time series analysis. These approaches can\n",
            "explicitly model inter-temporal and inter-variable relationships,\n",
            "which traditional and other deep neural network-based methods\n",
            "struggle to do. In this survey, we provide a comprehensive re-\n",
            "view of graph neural networks for time series analysis (GNN4TS),\n",
            "encompassing four fundamental dimensions: forecasting, classiﬁ-\n",
            "cation, anomaly detection, and imputation. Our aim is to guide\n",
            "designers and practitioners to understand, build applications, and\n",
            "advance research of GNN4TS. At ﬁrst, we provide a comprehensive\n",
            "task-oriented taxonomy of GNN4TS. Then, we present and discuss\n",
            "representative research works and introduce mainstream applica-\n",
            "tions of GNN4TS. A comprehensive discussion of potential future\n",
            "research directions completes the survey. This survey, for the ﬁrst\n",
            "time, brings together a vast array of knowledge on GNN-based time\n",
            "series research, highlighting foundations, practical applications,\n",
            "and opportunities of graph neural networks for time series analysis.\n",
            "Manuscript received 17 December 2023; revised 8 July 2024; accepted\n",
            "8 August 2024. Date of publication 14 August 2024; date of current version\n",
            "5 November 2024. This work was supported in part by the CSIRO– National Sci-\n",
            "ence Foundation (US) AI Research Collaboration Program and Swiss National\n",
            "Science Foundation under Grant 204061. The work of Shirui Pan was supported\n",
            "in part by the Australian Research Council (ARC) under Grant FT210100097\n",
            "and Grant DP240101547. Recommended for acceptance by W. Liu. (Ming Jin\n",
            "and Huan Yee Koh contributed equally to this work.) (Corresponding author:\n",
            "Shirui Pan.)\n",
            "Ming Jin and Shirui Pan are with the School of Information and Communi-\n",
            "cation Technology, Grifﬁth University, Nathan, QLD 4111, Australia (e-mail:\n",
            "mingjinedu@gmail.com; s.pan@grifﬁth.edu.au).\n",
            "Huan Yee Koh and Geoffrey I. Webb are with the Department of Data\n",
            "Science and AI, Monash University, Clayton, VIC 3800, Australia (e-mail:\n",
            "huan.koh@monash.edu; geoff.webb@monash.edu).\n",
            "Qingsong Wen is with Squirrel AI Learning, Bellevue, WA 98004 USA (e-\n",
            "mail: qingsongedu@gmail.com).\n",
            "Daniele Zambon is with the Swiss AI Lab IDSIA, Università della Svizzera\n",
            "Italiana, 6900 Lugano, Switzerland (e-mail: daniele.zambon@usi.ch).\n",
            "Cesare Alippi is with the Swiss AI Lab IDSIA, Università della Svizzera\n",
            "Italiana, 6900 Lugano, Switzerland, and also with Politecnico di Milano, 20133\n",
            "Milano, Italy (e-mail: cesare.alippi@usi.ch).\n",
            "Irwin King is with the Department of Computer Science & Engineer-\n",
            "ing, Chinese University of Hong Kong, Ma Liu Shui, Hong Kong (e-mail:\n",
            "king@cse.cuhk.edu.hk).\n",
            "This article has supplementary downloadable material available at\n",
            "https://doi.org/10.1109/TPAMI.2024.3443141, provided by the authors.\n",
            "Digital Object Identiﬁer 10.1109/TPAMI.2024.3443141Index Terms —Time series, graph neural networks, deep\n",
            "learning, forecasting, classiﬁcation, imputation, anomaly detection.\n",
            "I. I NTRODUCTION\n",
            "THE advent of advanced sensing and data stream processing\n",
            "technologies has led to an explosion of time series data [1],\n",
            "[2],[3]. The analysis of time series not only provides insights\n",
            "into past trends but also facilitates a multitude of tasks such\n",
            "as forecasting [4], classiﬁcation [5], anomaly detection [6],\n",
            "and data imputation [7]. This lays the groundwork for time\n",
            "series modeling paradigms that leverage on historical data to\n",
            "understand current and future possibilities. Time series analytics\n",
            "have become increasingly crucial in various ﬁelds, including but\n",
            "not limited to cloud computing, transportation, energy, ﬁnance,\n",
            "social networks, and the Internet-of-Things [8],[9],[10].\n",
            "Many time series involve complex interactions across time\n",
            "(such as lags in propagation of effects) and variables (such as\n",
            "the relationship among the variables representing neighboring\n",
            "trafﬁc sensors). By treating time points or variables as nodes and\n",
            "their relationships as edges, a model structured in the manner\n",
            "of a network or graph can effectively solve the task at hand by\n",
            "exploiting both data and relational information. Indeed, much\n",
            "time series data is spatial-temporal in nature, with different\n",
            "variables in the series capturing information about different\n",
            "locations – space – too, meaning it encapsulates not only time in-\n",
            "formation but also spatial relationships [11]. This is particularly\n",
            "evident in scenarios such as urban trafﬁc networks, population\n",
            "migration, and global weather forecasting. In these instances, a\n",
            "localized change, such as a trafﬁc accident at an intersection, an\n",
            "epidemic outbreak in a suburb, or extreme weather in a speciﬁc\n",
            "area, can propagate and inﬂuence neighboring regions. This\n",
            "spatial-temporal characteristic is a common feature of many\n",
            "dynamic systems, including the wind farm in Fig. 1, where\n",
            "the underlying time series displays a range of correlations and\n",
            "heterogeneities [15],[18]. Traditional analytic tools, such as\n",
            "support vector regression (SVR) [19], gradient boosting decision\n",
            "tree (GBDT) [20], vector autoregressive (V AR) [21], and au-\n",
            "toregressive integrated moving average (ARIMA) [22], struggle\n",
            "to handle complex time series relations (e.g., nonlinearities and\n",
            "inter-variable relationships), resulting in less accurate prediction\n",
            "0162-8828 © 2024 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\n",
            "See https://www.ieee.org/publications/rights/index.html for more information.\n",
            "Authorized licensed use limited to: Mapua University. Downloaded on April 02,2025 at 08:19:48 UTC from IEEE Xplore.  Restrictions apply. JIN et al.: SURVEY ON GRAPH NEURAL NETWORKS FOR TIME SERIES: FORECASTING, CLASSIFICATION, 10467\n",
            "Fig. 1. Graph neural networks for time series analysis (GNN4TS). In this\n",
            "example of a wind farm, different analytical tasks can be categorized into time\n",
            "series forecasting, classiﬁcation, anomaly detection, and imputation.\n",
            "results [23]. The advent of deep learning technologies has led\n",
            "to the development of different neural networks based on con-\n",
            "volutional neural networks (CNN) [24],[25], recurrent neural\n",
            "networks (RNN) [26], and transformers [27], which have shown\n",
            "signiﬁcant advantages in modeling real-world time series data.\n",
            "However, one of the biggest limitations of the above methods\n",
            "is that they do not explicitly model the spatial relations existing\n",
            "between time series in non-euclidean space [15], which limits\n",
            "their expressiveness [28].\n",
            "In recent years, graph neural networks (GNNs) have emerged\n",
            "as a powerful tool for learning non-euclidean data represen-\n",
            "tations [29],[30],[31],[32], paving the way for modeling\n",
            "real-world time series data. This enables the capture of diverse\n",
            "and intricate relationships, both inter-variable (connections be-\n",
            "tween different variables within a multivariate series) and inter-\n",
            "temporal (dependencies between different points in time). Con-\n",
            "sidering the complex spatial-temporal dependencies inherent in\n",
            "real-world scenarios, a line of studies has integrated GNNs with\n",
            "various temporal modeling frameworks to capture both spatial\n",
            "and temporal dynamics and demonstrate promising results [12],\n",
            "[13],[14],[15],[16]. While early research efforts were primarily\n",
            "concentrated on various forecasting scenarios [13],[14],[15],\n",
            "recent advancements in time series analysis utilizing GNNs have\n",
            "demonstrated promising outcomes in other mainstream tasks.\n",
            "These include classiﬁcation [33],[34], anomaly detection [35],\n",
            "[36], and imputation [37],[38].I nF i g . 1, we provide an overview\n",
            "of graph neural networks for time series analysis (GNN4TS).\n",
            "Related Surveys. Despite the growing body of research per-\n",
            "forming various time series analytic tasks with GNNs, existing\n",
            "surveys tend to focus on speciﬁc perspectives within a restricted\n",
            "scope. For instance, the survey by Wang et al. [11]offers a review\n",
            "of deep learning techniques for spatial-temporal data mining, but\n",
            "it does not speciﬁcally concentrate on GNN-based methods. The\n",
            "survey by Ye et al. [12] zeroes in on graph-based deep learning\n",
            "architectures in the trafﬁc domain, primarily considering fore-\n",
            "casting scenarios. A recent survey by Jin et al. [15] offers anoverview of GNNs for predictive learning in urban computing,\n",
            "but neither extends its coverage to other application domains nor\n",
            "thoroughly discusses other tasks related to time series analysis.\n",
            "Finally, we mention the work by Rahmani et al. [17], which\n",
            "expands the survey of GNNs to many intelligent transportation\n",
            "systems, but tasks other than forecasting remain overlooked. A\n",
            "detailed comparison between our survey and others is presented\n",
            "in Table I.\n",
            "To ﬁll the gap, this survey offers a comprehensive and up-to-\n",
            "date review of graph neural networks for time series analysis,\n",
            "encompassing the majority of tasks ranging from time series\n",
            "forecasting, classiﬁcation, anomaly detection, and imputation.\n",
            "Speciﬁcally, we ﬁrst provide two broad views to classify and\n",
            "discuss existing works from the task- and methodology-oriented\n",
            "perspectives. Then, we delve into six popular application sectors\n",
            "within the existing research of GNN4TS, and propose several\n",
            "potential future research directions. The key contributions of our\n",
            "survey are summarized as follows:rFirst Comprehensive Survey: To the best of our knowledge,\n",
            "this is the ﬁrst comprehensive survey that reviews the recent\n",
            "advances in mainstream time series analysis tasks with\n",
            "graph neural networks. It covers a wide range of recent\n",
            "research and provides a broad view of the development of\n",
            "GNN4TS without restricting to speciﬁc tasks or domains.rUniﬁed and Structured Taxonomy: We present a uniﬁed\n",
            "framework to structurally categorize existing works from\n",
            "task- and methodology-oriented perspectives. In the ﬁrst\n",
            "classiﬁcation, we offer an overview of tasks in time series\n",
            "analysis, covering different problem settings prevalent in\n",
            "GNN-based research; and in the second classiﬁcation, we\n",
            "dissect GNN4TS in terms of spatial and temporal depen-\n",
            "dencies modeling and the overall model architecture.rDetailed and Current Overview: We conduct a compre-\n",
            "hensive review that not only covers the breadth of the\n",
            "ﬁeld but also delves into the depth of individual studies\n",
            "with ﬁne-grained classiﬁcation and detailed discussion,\n",
            "providing readers with an up-to-date understanding of the\n",
            "state-of-the-art in GNN4TS.rBroadening Applications: We discuss the expanding appli-\n",
            "cations of GNN4TS across various sectors, highlighting its\n",
            "versatility and potential for future growth in diverse ﬁelds.rFuture Research Directions: We shed light on potential fu-\n",
            "ture research directions, offering insights and suggestions\n",
            "that could guide and inspire future research in the ﬁeld of\n",
            "GNN4TS.\n",
            "The remainder of this survey is organized as follows:\n",
            "Section IIprovides notations used throughout the paper.\n",
            "Section IIIpresents the taxonomy of GNN4TS from different\n",
            "perspectives. Sections IV,V,VI, and VIIreview the four major\n",
            "tasks in the GNN4TS literature. Section VIII surveys popular\n",
            "applications of GNN4TS across various ﬁelds, while Section IX\n",
            "examines open questions and potential future directions.\n",
            "II. D EFINITION AND NOTATION\n",
            "Time series data comprises a sequence of observations gath-\n",
            "ered or recorded over a period of time. This data can be either\n",
            "Authorized licensed use limited to: Mapua University. Downloaded on April 02,2025 at 08:19:48 UTC from IEEE Xplore.  Restrictions apply. 10468 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 46, NO. 12, DECEMBER 2024\n",
            "TABLE I\n",
            "COMPARISON BETWEEN OURSURVEY AND OTHER RELATED SURVEYS\n",
            "regularly orirregularly sampled , with the latter also referred to\n",
            "as time series data with missing values. Within each of these\n",
            "cases, the data can be further classiﬁed into two primary types:\n",
            "univariate andmultivariate time series . In the sequel, we employ\n",
            "bold uppercase letters (e.g., X), bold lowercase letters (e.g., x),\n",
            "and calligraphic letters (e.g., V) to denote matrices, vectors, and\n",
            "sets, respectively.\n",
            "Deﬁnition 1 (Univariate Time Series): A univariate time se-\n",
            "ries is a sequence of scalar observations collected over time,\n",
            "which can be regularly or irregularly sampled. A regularly sam-\n",
            "pled univariate time series is deﬁned as X={x1,x2,...,x T}∈\n",
            "RT, where xt∈R. For an irregularly sampled univariate\n",
            "time series, observations are collected at non-uniform time\n",
            "intervals, such as X={(t1,x1),(t2,x2),...,(tT,xT)}∈RT,\n",
            "where time points are non-uniformly spaced.\n",
            "Deﬁnition 2 (Multivariate Time Series): A multivariate time\n",
            "series is a sequence of N-dimensional vector observations col-\n",
            "lected over time, i.e., X∈RN×T. A regularly sampled mul-\n",
            "tivariate time series has vector observations collected at uni-\n",
            "form time intervals, i.e., xt∈RN. In an irregularly sampled\n",
            "multivariate time series, there are possibly Nunaligned time\n",
            "series with respect to time steps, which implies only 0≤n≤N\n",
            "observations available at each time step.\n",
            "The majority of research based on GNNs focuses on modeling\n",
            "multivariate time series, as they can be naturally abstracted into\n",
            "spatial-temporal graphs . This abstraction allows for an accurate\n",
            "characterization of dynamic inter-temporal and inter-variable\n",
            "dependencies. The former describes the relations between dif-\n",
            "ferent time steps within each time series (e.g., the temporal\n",
            "dynamics of red nodes between t1andt3in Fig. 2), while the\n",
            "latter captures dependencies between time series (e.g., the spatial\n",
            "relations between four nodes at each time step in Fig. 2), such as\n",
            "the geographical information of the sensors generating the data\n",
            "for each variable. To illustrate this, we ﬁrst deﬁne attributed\n",
            "graphs .\n",
            "Deﬁnition 3 (Attributed Graph): An attributed graph is a\n",
            "static graph that associates each node with a set of attributes,\n",
            "representing node features. Formally, an attributed graph is de-\n",
            "ﬁned asG=(A,X), which consists of a (weighted) adjacency\n",
            "matrix A∈RN×Nand a node-feature matrix X∈RN×D.T h e\n",
            "adjacency matrix represents the graph topology, which can beFig. 2. Examples of spatial-temporal graphs, where node colors represent\n",
            "distinct features. The top and bottom panels demonstrate spatio-temporal graphs\n",
            "with ﬁxed and dynamic graph structures over time, respectively.\n",
            "characterized by V={v1,v2,...,v N},t h es e to f Nnodes, and\n",
            "E={eij:= (vi,vj)∈V×V| Aij/negationslash=0}, the set of edges; Aij\n",
            "is the(i,j)-th entry in the adjacency matrix A. The feature ma-\n",
            "trixXcontains the node attributes, where the i-th row xi∈RD\n",
            "represents the D-dimensional feature vector of node vi.\n",
            "In attributed graphs, multi-dimensional edge features can be\n",
            "considered too, however, this paper assumes only scalar weights\n",
            "encoded in the adjacency matrix to avoid overwhelming nota-\n",
            "tions.\n",
            "In light of this, a spatial-temporal graph can be described as\n",
            "a series of attributed graphs, which effectively represent (multi-\n",
            "variate) time series data in conjunction with either evolving or\n",
            "ﬁxed structural information over time.\n",
            "Deﬁnition 4 (Spatial-Temporal Graph): A spatial-temporal\n",
            "graph can be interpreted as a discrete-time dynamic graph [31],\n",
            "[39], i.e.,G={G1,G2,...,GT}, whereGt=(At,Xt)denotes\n",
            "an attributed graph at time t.At∈RN×NandXt∈RN×Dare\n",
            "corresponding adjacency and feature matrices. Atmay either\n",
            "evolve over time or remain ﬁxed, depending on speciﬁc settings.\n",
            "When abstracting time series data, we let Xt:=xt∈RN.\n",
            "We introduce graph neural networks as modern deep learning\n",
            "models to process graph-structured data. The core operation in\n",
            "typical GNNs, often referred to as graph convolution ,i n v o l v e s\n",
            "exchanging information across neighboring nodes. In the context\n",
            "of time series analysis, this operation enables us to explicitly\n",
            "Authorized licensed use limited to: Mapua University. Downloaded on April 02,2025 at 08:19:48 UTC from IEEE Xplore.  Restrictions apply. JIN et al.: SURVEY ON GRAPH NEURAL NETWORKS FOR TIME SERIES: FORECASTING, CLASSIFICATION, 10469\n",
            "rely on the inter-variable dependencies represented by the graph\n",
            "edges. Aware of the different nuances, we deﬁne GNNs in the\n",
            "spatial domain, which involves transforming the input signal\n",
            "with learnable functions along the dimension of N.\n",
            "Deﬁnition 5 (Graph Neural Network): Given an attributed\n",
            "graphG=(A,X), we deﬁne xi=X[i,:]∈RDas theD-\n",
            "dimensional feature vector of node vi. A GNN learns node rep-\n",
            "resentations through two primary functions [40]:AGGREGATE (·)\n",
            "and C OMBINE(·).T h eA GGREGATE (·)function computes and\n",
            "aggregates messages from neighboring nodes, while the\n",
            "COMBINE(·)function merges the aggregated and previous states\n",
            "to transform node embeddings. Formally, the k-th layer in a\n",
            "GNN is deﬁned by the extended\n",
            "a(k)\n",
            "i=AGGREGATE(k)/parenleftBig/braceleftBig\n",
            "h(k−1)\n",
            "j:vj∈N(vi)/bracerightBig/parenrightBig\n",
            ",\n",
            "h(k)\n",
            "i=COMBINE(k)/parenleftBig\n",
            "h(k−1)\n",
            "i,a(k)\n",
            "i/parenrightBig\n",
            ", (1)\n",
            "or, more generally, aggregating messages computed from both\n",
            "sending and receiving nodes vjandvi, respectively. Here, a(k)\n",
            "i\n",
            "andh(k)\n",
            "irepresent the aggregated message from neighbors and\n",
            "the transformed node embedding of node viin thek-th layer,\n",
            "respectively. The input and output of a GNN are h(0)\n",
            "i:=xiand\n",
            "h(K)\n",
            "i:=hi.\n",
            "The above formulation in (1)is referred to as spatial GNNs ,\n",
            "as opposed to spectral GNNs which deﬁnes convolution from\n",
            "the lens of spectral graph theory. We refer the reader to recent\n",
            "publication [28] for a deeper analysis of spectral versus spatial\n",
            "GNNs, and [29] for a comprehensive review of GNNs.\n",
            "To employ GNNs for time series analysis, it is implied that a\n",
            "graph structure must be provided. However, not all time series\n",
            "data have readily available graph structures and, in practice, two\n",
            "types of strategies are utilized to generate the missing graph\n",
            "structures from the data: heuristics orlearned from data.\n",
            "Heuristic-Based Graphs: This group of methods extracts\n",
            "graph structures from data based on heuristics, such as:rSpatial Proximity: This approach deﬁnes the graph struc-\n",
            "ture by considering the proximity between pairs of nodes\n",
            "based on, e.g., their geographical location. A typical exam-\n",
            "ple is the construction of the adjacency matrix Abased on\n",
            "the shortest travel distance between nodes when the time\n",
            "series data have geospatial properties:\n",
            "Ai,j=/braceleftbigg1\n",
            "dij,ifdij/negationslash=0,\n",
            "0, otherwise ,(2)\n",
            "wheredijdenotes the shortest travel distance between\n",
            "nodeiand node j. Some common kernel functions, e.g.,\n",
            "Gaussian radial basis, can also be applied [15].rPairwise Connectivity: In this approach, the graph structure\n",
            "is determined by the connectivity between pairs of nodes,\n",
            "like that determined by transportation networks. The adja-\n",
            "cency matrix Ais deﬁned as:\n",
            "Ai,j=/braceleftbigg\n",
            "1,ifviandvjare directly linked ,\n",
            "0,otherwise .(3)\n",
            "Typical scenarios include edges representing roads, rail-\n",
            "ways, or adjacent regions [41],[42]. In such cases, thegraph can be undirected or directed, resulting in symmetric\n",
            "and asymmetric adjacency matrices.rPairwise Similarity: This method constructs the graph by\n",
            "connecting nodes with similar attributes. A simple example\n",
            "is the construction of adjacency matrix Abased on the\n",
            "cosine similarity between time series:\n",
            "Ai,j=x/latticetop\n",
            "ixj\n",
            "/bardblxi/bardbl/bardblxj/bardbl, (4)\n",
            "where/bardbl·/bardbl denotes the euclidean norm. There are also\n",
            "several variants for creating similarity-based graphs, such\n",
            "as Pearson correlation coefﬁcient (PCC) [43] and dynamic\n",
            "time warping (DTW) [44].rFunctional Dependence: This approach deﬁnes the graph\n",
            "structure based on known functional dependencies be-\n",
            "tween pairs of nodes, such as direct causal relationships\n",
            "or dependence from common hidden factors. For instance,\n",
            "adjacency matrix Acan be constructed based on Granger\n",
            "causality [45] as\n",
            "Ai,j=⎧\n",
            "⎨\n",
            "⎩1,if nodejGranger-causes\n",
            "nodeiat a signiﬁcance level α,\n",
            "0,otherwise .(5)\n",
            "Other examples involve transfer entropy (TE) [46] and\n",
            "directed phase lag index (DPLI) [47]. While overlap with\n",
            "previous heuristics exists, functional relations typically\n",
            "represent or estimate actual node dependencies in the data-\n",
            "generating process.\n",
            "Learning-Based Graphs: In contrast to heuristic-based meth-\n",
            "ods, learning-based approaches aim to learn the graph structure\n",
            "directly from the data and end-to-end with the downstream task.\n",
            "Accordingly, graph Acan be deﬁned as a function ρ(·)of some\n",
            "trainable model parameters Θand, possibly, also of the time\n",
            "series observations X, i.e.,A=ρ(Θ,X). Embedding-based\n",
            "approaches (e.g., [48],[49]) deﬁne the presence of each edge by\n",
            "comparing learned embedding vectors of the associated nodes;\n",
            "as an example, consider Ai,j=ReLU(Θ/latticetop\n",
            "iΘj)withΘi,Θj\n",
            "node embedding of nodes i,j, respectively. Another common\n",
            "example involves deﬁning Athrough attention scores computed\n",
            "between node signals [50]. Sparsiﬁcation methods – such as the\n",
            "ReLU activation above or top-k selection – are often applied\n",
            "to discard edges with null or low weights, thereby reducing the\n",
            "computational load requested by dense GNN operations [51].A n\n",
            "alternative strategy is modeling Aas a discrete random variable\n",
            "whose parametric distribution, say pΘ(A|X), is learned along-\n",
            "side the other model parameters [52],[53]. Learning-based ap-\n",
            "proaches enable the data-driven discovery of less obvious graph\n",
            "structures that are tailored to solve the given task, potentially\n",
            "providing more effective relations than heuristic-based graphs.\n",
            "III. F RAMEWORK AND CATEGORIZATION\n",
            "In this section, we present a comprehensive task-oriented\n",
            "taxonomy for GNNs within the context of time series analysis\n",
            "(Section III-A ). Subsequently, we investigate how to encode time\n",
            "series across various tasks by introducing a uniﬁed methodologi-\n",
            "cal framework for GNN architectures (Section III-B ). According\n",
            "Authorized licensed use limited to: Mapua University. Downloaded on April 02,2025 at 08:19:48 UTC from IEEE Xplore.  Restrictions apply. 10470 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 46, NO. 12, DECEMBER 2024\n",
            "Fig. 3. Task-oriented taxonomy of graph neural networks for time series analysis in the existing literature.\n",
            "to the framework, all architectures are composed of a similar\n",
            "graph-based processing module fθand a second module pφ\n",
            "specialized in downstream tasks.\n",
            "A. Task-Oriented Taxonomy\n",
            "In Fig. 3, we illustrate a task-oriented taxonomy of GNNs\n",
            "encompassing the primary tasks and mainstream modeling for\n",
            "time series analysis, and showcasing the potential of GNN4TS.\n",
            "This survey focuses on four categories: time series forecasting ,\n",
            "anomaly detection ,imputation , and classiﬁcation . These tasks\n",
            "are performed on top of the time series representations learned\n",
            "byspatial-temporal graph neural networks (STGNNs), which\n",
            "serve as the foundation for encoding time series data in existing\n",
            "literature across various tasks. We detail this in Section III-B .\n",
            "Time Series Forecasting: This task is centered around pre-\n",
            "dicting future values of the time series based on historical\n",
            "observations, as depicted in Fig. 4(a). Depending on application\n",
            "needs, we categorize this task into two types: single-step-ahead\n",
            "forecasting and multi-step-ahead forecasting . The former is\n",
            "meant to predict single future observations of the time series\n",
            "once at a time, i.e., the target at time tisY:=Xt+Hfor some\n",
            "H∈Nsteps ahead, while the latter makes predictions for a time\n",
            "interval, e.g., Y:=Xt+1:t+H. Parameterized solutions to bothpredictive cases can be derived by optimizing\n",
            "θ∗,φ∗=a r gm i n\n",
            "θ,φLF(pφ(fθ(Xt−T:t,At−T:t)),Y), (6)\n",
            "wherefθ(·)andpφ(·)represent a spatial-temporal GNN and the\n",
            "predictor, respectively. Details regarding the fθ(·)architecture\n",
            "are given in Section III-B while the predictor is, normally, a\n",
            "multi-layer perceptron. In the sequel, we denote by Xt−T:tand\n",
            "At−T:ta spatial-temporal graph G={Gt−T,Gt−T+1,...,Gt}\n",
            "with length T. If the underlying graph structure is ﬁxed, then\n",
            "At:=A.LF(·)denotes the forecasting loss, which is typi-\n",
            "cally a squared or absolute loss function, e.g., STGCN [54]\n",
            "and MTGNN [51]. Most existing works minimize the error\n",
            "between the forecasting and the ground truth Ythrough (6);\n",
            "this process is known as deterministic time series forecasting.\n",
            "Besides, we have probabilistic time series forecasting methods,\n",
            "such as DiffSTG [55], that share the same objective (6)function\n",
            "though it is not directly optimized. Based on the size of the fore-\n",
            "casting horizon H, we end up in either short-term orlong-term\n",
            "forecasting .\n",
            "Time Series Anomaly Detection: This task focuses on de-\n",
            "tecting irregularities and unexpected events in time series data\n",
            "(Fig. 4(b)). Detecting anomalies requires determining when\n",
            "the anomalous event occurred, while diagnosing them requests\n",
            "gaining insights about how and why the anomaly occurred. Due\n",
            "Authorized licensed use limited to: Mapua University. Downloaded on April 02,2025 at 08:19:48 UTC from IEEE Xplore.  Restrictions apply. JIN et al.: SURVEY ON GRAPH NEURAL NETWORKS FOR TIME SERIES: FORECASTING, CLASSIFICATION, 10471\n",
            "Fig. 4. Four categories of graph neural networks for time series analysis. For the sake of simplicity and illustrative purposes, we assume the graph st ructures are\n",
            "ﬁxed in all subplots.\n",
            "to the general difﬁculty of acquiring anomaly events, current\n",
            "research commonly treats anomaly detection as an unsuper-\n",
            "vised problem that involves the design of a model describing\n",
            "normal, non-anomalous data. The learned model is then used\n",
            "to detect anomalies by generating a high score whenever an\n",
            "anomaly event occurs. This model learning process mirrors the\n",
            "forecasting optimization, (6), withfθ(·)andpφ(·)denote the\n",
            "spatial-temporal GNN and the predictor, respectively. In general,\n",
            "the spatial-temporal GNN and the predictor are trained on nor-\n",
            "mal, non-anomalous data using either forecasting [36],[56] or\n",
            "reconstruction [35],[57] optimization approaches, with the aim\n",
            "of minimizing the discrepancy between the normal input and the\n",
            "forecast (or reconstructed) series. Nonetheless, when these mod-\n",
            "els are put to use for detecting anomalies, they are expected to fail\n",
            "in minimizing this discrepancy upon receiving anomalous input.\n",
            "This inability to conform to the expected low-discrepancy model\n",
            "behavior during anomaly periods creates a detectable difference,\n",
            "facilitating the detection of anomalies. The threshold separating\n",
            "normal and anomalous data is a sensitive hyperparameter that\n",
            "should be set considering the rarity of anomalies and aligned\n",
            "with a desired false alarm rate [58]. Lastly, to diagnose the\n",
            "causes of anomalies, a common strategy involves calculatingdiscrepancies for each channel node and consolidating these\n",
            "into a single anomaly score [59]. This approach allows for\n",
            "the identiﬁcation of the channel variables responsible for the\n",
            "anomaly events by calculating their respective contributions to\n",
            "the ﬁnal score.\n",
            "Time Series Imputation: This task is centered around estimat-\n",
            "ing and ﬁlling in missing or incomplete data points within a time\n",
            "series (Fig. 4(c)). Current research in this domain can be broadly\n",
            "classiﬁed into two main approaches: in-sample imputation and\n",
            "out-of-sample imputation . In-sample imputation involves ﬁlling\n",
            "missing values in a given time series, while out-of-sample im-\n",
            "putation pertains to inferring missing data not present in the\n",
            "training dataset. We formulate the learning objective as follows:\n",
            "θ∗,φ∗=a r gm i n\n",
            "θ,φLI/parenleftBig\n",
            "pφ/parenleftBig\n",
            "fθ(˜Xt−T:t,At−T:t)/parenrightBig\n",
            ",Xt−T:t/parenrightBig\n",
            ",\n",
            "(7)\n",
            "wherefθ(·)andpφ(·)denote the spatial-temporal GNN and\n",
            "imputation module to be learned, respectively. The imputation\n",
            "module can e.g., be a multi-layer perceptron. In this task, ˜Xt−T:t\n",
            "represents input time series data with missing values (reference\n",
            "time series), while Xt−T:tdenotes the same time series without\n",
            "Authorized licensed use limited to: Mapua University. Downloaded on April 02,2025 at 08:19:48 UTC from IEEE Xplore.  Restrictions apply. 10472 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 46, NO. 12, DECEMBER 2024\n",
            "missing values. As it is impossible to access the reference\n",
            "time series during training, a surrogate optimization objective is\n",
            "considered, such as generating synthetic missing values [37].I n\n",
            "(7),LI(·)refers to the imputation loss, which can be, for instance,\n",
            "an absolute or a squared error, similar to forecasting tasks. For in-\n",
            "sample imputation, the model is trained and evaluated on ˜Xt−T:t\n",
            "andXt−T:t. Instead, for out-of-sample imputation, the model\n",
            "is trained and evaluated on disjoint sequences, e.g., trained on\n",
            "˜Xt−T:tbut evaluated on Xt:t+H, where the missing values in\n",
            "˜Xt:t+Hwill be estimated. Similar to time series forecasting\n",
            "and anomaly detection, the imputation process can be either\n",
            "deterministic orprobabilistic . The former predicts the missing\n",
            "values directly (e.g., GRIN [37]), while the latter estimates the\n",
            "missing values from data distributions (e.g., PriSTI [38]).\n",
            "Time Series Classiﬁcation: This task aims to assign a cat-\n",
            "egorical label to a given time series based on its underlying\n",
            "patterns or characteristics. Rather than capturing patterns within\n",
            "a time series data sample, the essence of time series classiﬁcation\n",
            "resides in discerning differentiating patterns that help separate\n",
            "samples based on their class labels. The optimization problem\n",
            "can be expressed as:\n",
            "θ∗,φ∗=a r gm i n\n",
            "θ,φLC(pφ(fθ(X,A)),Y), (8)\n",
            "wherefθ(·)andpφ(·)denote, e.g., a GNN and a classiﬁer to\n",
            "be learned, respectively. Using univariate time series classiﬁ-\n",
            "cation as an example Fig. 4(d), the task can be formulated as\n",
            "either a graph or node classiﬁcation task. In the case of graph\n",
            "classiﬁcation ( Series-as-Graph )[96], each series is transformed\n",
            "into a graph, and the graph will be the input of a GNN to\n",
            "generate a classiﬁcation output. This can be achieved by dividing\n",
            "a series into multiple subsequences with a window size, W,\n",
            "serving as graph nodes, X∈RN×W, and an adjacency matrix,\n",
            "A, describing the relationships between subsequences. A simple\n",
            "GNN,fθ(·), then employs graph convolution and pooling to\n",
            "obtain a condensed graph feature to be exploited by a classiﬁer\n",
            "pφ(·)which assigns a class label to the graph. Alternatively,\n",
            "the node classiﬁcation formulation ( Series-as-Node ), treats each\n",
            "series as a node in a dataset graph. Series-as-Node constructs an\n",
            "adjacency matrix representing the relationships between mul-\n",
            "tiple distinct series in a given dataset [63]. With several series\n",
            "of length Tstacked into a matrix X∈RN×Tas node features\n",
            "andArepresenting pairwise relationships, the GNN operation,\n",
            "fθ(·), aims at leveraging the relationships across different series\n",
            "for accurate node series classiﬁcation [97]. In all cases, Yis\n",
            "typically a one-hot encoded vector representing the categorical\n",
            "label of a univariate or multivariate time series.\n",
            "B. Uniﬁed Methodological Framework\n",
            "In Fig. 5, we present a uniﬁed methodological framework\n",
            "of STGNNs mentioned in Section III-A for time series analysis.\n",
            "Speciﬁcally, our framework serves as the basis for encoding time\n",
            "series data in the existing literature across various downstream\n",
            "tasks (Fig. 3). As an extension, STGNNs incorporate spatial\n",
            "information by considering the relationships between nodes in\n",
            "the graph and temporal information by taking into account theFig. 5. Methodology-oriented taxonomy of graph neural networks for time\n",
            "series analysis.\n",
            "evolution of node attributes over time. Similar to [15],w es y s -\n",
            "tematically categorize STGNNs from three perspectives: spatial\n",
            "module ,temporal module , and overall model architecture .\n",
            "Spatial Module: To model dependencies between time series\n",
            "over time, STGNNs employ the design principles of GNNs on\n",
            "static graphs. These can be further categorized into three types:\n",
            "spectral GNNs ,spatial GNNs , and a combination of both (i.e.,\n",
            "hybrid )[29]. Spectral GNNs are based on spectral graph theory\n",
            "and use the graph shift operator (like the graph Laplacian) to\n",
            "capture node relationships in the graph frequency domain [28],\n",
            "[98],[99]. Differently, spatial GNNs simplify spectral GNNs\n",
            "by directly designing ﬁlters that are localized to each node’s\n",
            "neighborhood. The approaches in this category can be broadly\n",
            "classiﬁed into two types: graph diffusion-based and message\n",
            "passing-based . Notably, graph transformers [100] represent a\n",
            "specialized extension of message passing neural networks [101] ,\n",
            "further expanding the capabilities of this paradigm. Hybrid\n",
            "approaches combine both spectral and spatial methodologies\n",
            "to capitalize on the strengths of each method.\n",
            "Temporal Module: To account for temporal dependencies\n",
            "in time series, STGNNs incorporate temporal modules that\n",
            "work in tandem with spatial modules to model intricate spatial-\n",
            "temporal patterns. Temporal dependencies can be represented\n",
            "in either the time orfrequency domains. The approaches in the\n",
            "ﬁrst category encompass recurrence-based (e.g., RNNs [26]),\n",
            "convolution-based (e.g., TCNs [102] ),attention-based (e.g.,\n",
            "transformers [27]), and a combination of these (i.e., hybrid ).\n",
            "For the second category, analogous techniques are employed,\n",
            "followed by orthogonal space projections [28], such as the\n",
            "Fourier transform.\n",
            "Model Architecture: To integrate the two modules, existing\n",
            "STGNNs are either discrete orcontinuous in terms of their\n",
            "overall neural architectures. Both types can be further subdivided\n",
            "into two subcategories: factorized and coupled . With typical\n",
            "Authorized licensed use limited to: Mapua University. Downloaded on April 02,2025 at 08:19:48 UTC from IEEE Xplore.  Restrictions apply. JIN et al.: SURVEY ON GRAPH NEURAL NETWORKS FOR TIME SERIES: FORECASTING, CLASSIFICATION, 10473\n",
            "Fig. 6. General pipeline for time series analysis using graph neural networks.\n",
            "factorized STGNN model architectures, the temporal processing\n",
            "is performed either before or after the spatial processing, whether\n",
            "in a discrete (e.g., STGCN [54]) or continuous manner (e.g.,\n",
            "STGODE [74]). Conversely, the coupled model architecture\n",
            "refers to instances where spatial and temporal modules are\n",
            "interleaved, such as DCRNN [68] (discrete) and MTGODE [23]\n",
            "(continuous). Other authors refer to very related categories as\n",
            "time-then-space and time-and-space [103] .\n",
            "General Pipeline: In Fig. 6, we showcase a general pipeline\n",
            "that shows how STGNNs can be integrated into time series\n",
            "analysis. Given a time series dataset, we ﬁrst process it using the\n",
            "data processing module , which performs essential data cleaning\n",
            "and normalization tasks, including the extraction of time series\n",
            "topology (i.e., graph structures). Subsequently, STGNNs are\n",
            "utilized to obtain time series representations, which can then\n",
            "be passed to different handlers (i.e., downstream task prediction\n",
            "module ) to execute various analytical tasks, such as forecasting\n",
            "and anomaly detection.\n",
            "IV . GNN S FOR TIMESERIES FORECASTING\n",
            "Time series forecasting aims to predict future time series\n",
            "values based on historical observations. While deep learning\n",
            "models have demonstrated considerable success in forecasting\n",
            "time series by capturing nonlinear temporal and spatial patterns\n",
            "more effectively than the linear counterpart [23], many of these\n",
            "approaches, such as LSTNet [111] and TPA-LSTM [112] , over-\n",
            "look and implicitly model the rich underlying dynamic spatial\n",
            "correlations between time series. Recently, graph neural network\n",
            "(GNN)-based methods have shown great potential in explicitly\n",
            "and effectively modeling spatial and temporal dependencies in\n",
            "multivariate time series data, leading to enhanced forecasting\n",
            "performance.\n",
            "GNN-based forecasting models can be categorized and ex-\n",
            "amined from multiple perspectives. In terms of forecasting\n",
            "tasks, while many models focus on multi-step forecasting (i.e.,\n",
            "predicting multiple consecutive steps ahead based on historical\n",
            "observations), a minority also discuss single-step forecasting\n",
            "(i.e., predicting the next or one arbitrary step ahead). From\n",
            "a methodological standpoint, these models can be dissected\n",
            "from three aspects: (1) modeling spatial (i.e., inter-variable)\n",
            "dependencies, (2) modeling inter-temporal dependencies, and\n",
            "(3) the fusion of spatial and temporal modules for time series\n",
            "forecasting. A summary of representative works is given in\n",
            "Table II.A. Modeling Inter-Variable Dependencies\n",
            "Spatial dependencies, or inter-variable relationships, play a\n",
            "pivotal role in affecting a model’s forecasting capability [28].\n",
            "When presented with time series data and corresponding graph\n",
            "structures that delineate the strength of interconnections be-\n",
            "tween time series, current studies typically employ (1) spectral\n",
            "GNNs ,( 2 ) spatial GNNs ,o r( 3 )a hybrid of both to model\n",
            "these spatial dependencies. At a high level, these methods all\n",
            "draw upon the principles of graph signal processing (as detailed\n",
            "in Deﬁnition 5and subsequent discussion). Considering input\n",
            "variables XtandAtat a given time t, the goal here is to devise\n",
            "an effective GNN-based model S PATIAL(·)to adeptly capture\n",
            "salient patterns from different time series. This can be expressed\n",
            "asˆXt=SPATIAL(Xt,At), whereˆXtcollects all time series\n",
            "representations at time twith spatial dependencies embedded.\n",
            "Spectral GNN-Based Approaches: Early GNN-based fore-\n",
            "casting models predominantly utilized ChebConv [113] to ap-\n",
            "proximate graph convolution with Chebyshev polynomials,\n",
            "thereby modeling inter-variable dependencies. For instance,\n",
            "STGCN [54] intersects temporal convolution [114] and Cheb-\n",
            "Conv layers to capture both spatial and temporal patterns.\n",
            "StemGNN [50] further proposes spectral-temporal graph neu-\n",
            "ral networks to extract rich time series patterns by leveraging\n",
            "ChebConv and frequency-domain convolution neural networks.\n",
            "Other relevant research has largely followed suit, employing\n",
            "ChebConv to model spatial time series dependencies, while\n",
            "introducing novel modiﬁcations. These include attention mech-\n",
            "anisms [86],[105] , multi-graph construction [41],[72], and\n",
            "combinations of the two [88].\n",
            "Spatial GNN-Based Approaches: Inspired by the recent suc-\n",
            "cess of spatial GNNs [29], another line of research has been mod-\n",
            "eling inter-variable dependencies using message passing [115]\n",
            "or graph diffusion [116] . From the graph perspective, these\n",
            "methods are certain simpliﬁcations compared to those based\n",
            "on spectral GNNs, where strong local homophilies are em-\n",
            "phasised [28],[117] . Early methods such as DCRNN [68]\n",
            "and Graph WaveNet [48] incorporated graph diffusion lay-\n",
            "ers into GRU [118] or temporal convolution to model time\n",
            "series data. In contrast, STGCN (1st)(a second version of\n",
            "STGCN [54]) and ST-MetaNet [76] modeled spatial dependen-\n",
            "cies with GCN [119] and GAT [120] to aggregate information\n",
            "from adjacent time series. To enhance learning capabilities,\n",
            "several improvements have been proposed. For example, STS-\n",
            "GCN [92] proposed spatial-temporal synchronous graph convo-\n",
            "lution, extending GCN to model spatial and temporal dependen-\n",
            "cies on localized spatial-temporal graphs. MTGODE [23] and\n",
            "TPGNN [60] proposed continuous graph propagation and graph\n",
            "propagation based on temporal polynomial coefﬁcients. Addi-\n",
            "tionally, recent approaches based on graph transformer [100]\n",
            "or hypergraphs [121] , such as [122] and [110] , can capture\n",
            "longer-range spatial dependencies due to their global receptive\n",
            "ﬁeld, making them a separate branch of enhanced methods.\n",
            "Hybrid Approaches: Some hybrid models also exist, integrat-\n",
            "ing both spectral and spatial GNNs. For instance, SLCNN [95]\n",
            "employs ChebConv and localized message passing as global\n",
            "and local convolutions to capture spatial relations at multiple\n",
            "Authorized licensed use limited to: Mapua University. Downloaded on April 02,2025 at 08:19:48 UTC from IEEE Xplore.  Restrictions apply. 10474 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 46, NO. 12, DECEMBER 2024\n",
            "TABLE II\n",
            "SUMMARY OF REPRESENTATIVE GRAPH NEURAL NETWORKS FOR TIMESERIES FORECASTING\n",
            "TABLE III\n",
            "SUMMARY OF REPRESENTATIVE GRAPH NEURAL NETWORKS FOR TIMESERIES ANOMALY DETECTION\n",
            "Authorized licensed use limited to: Mapua University. Downloaded on April 02,2025 at 08:19:48 UTC from IEEE Xplore.  Restrictions apply. JIN et al.: SURVEY ON GRAPH NEURAL NETWORKS FOR TIME SERIES: FORECASTING, CLASSIFICATION, 10475\n",
            "TABLE IV\n",
            "SUMMARY OF GRAPH NEURAL NETWORKS FOR TIMESERIES CLASSIFICATION\n",
            "granularities. Conversely, Auto-STGNN [123] integrates neural\n",
            "architecture search to identify high-performance GNN-based\n",
            "forecasting models.\n",
            "More details about modeling inter-variable dependencies in\n",
            "GNNs for time series forecasting are in Appendix A.1 , available\n",
            "online.\n",
            "B. Modeling Inter-Temporal Dependencies\n",
            "The modeling of temporal dependencies within time series\n",
            "represents another important element in various GNN-based\n",
            "forecasting methods. These dependencies (i.e., temporal pat-\n",
            "terns) are capable of being modeled in the time or/and frequency\n",
            "domains. A summary of representative methods, along with\n",
            "their temporal module classiﬁcations, is presented in Table II.\n",
            "Given a univariate time series Xnwith length T, the primary\n",
            "goal here is to learn an effective temporal model, referred to\n",
            "as T EMPORAL (·). This model is expected to accurately capture\n",
            "the dependencies between data points within Xn, such that\n",
            "ˆXn=TEMPORAL (Xn), whereˆXnsymbolizes the representa-\n",
            "tion of time series Xn. In the construction of T EMPORAL (·),\n",
            "both the time and frequency domains can be exploited within\n",
            "convolutional andattentive mechanisms. Recurrent models can\n",
            "also be employed for modeling in the time domain speciﬁcally.\n",
            "Additionally, hybrid models exist in both domains, integrating\n",
            "different methodologies such as attention and convolution neural\n",
            "networks.\n",
            "Recurrent Models: Several early methodologies rely on re-\n",
            "current models for understanding inter-temporal dependencies\n",
            "in the time domain. For instance, DCRNN [68] integrates graph\n",
            "diffusion with gated recurrent units (GRU) [118] to model the\n",
            "spatial-temporal dependencies in trafﬁc data. On a different\n",
            "note, AGCRN [49] merges GRU with a factorized variant of\n",
            "GCN [119] and a graph structure learning module. More recent\n",
            "studies, such as GTS [52] and RGSL [72], share similar de-\n",
            "signs but primarily emphasize different graph structure learning\n",
            "mechanisms.\n",
            "Convolution Models: Convolutional neural networks (CNNs),\n",
            "on the other hand, provide a more efﬁcient perspective for\n",
            "modeling inter-temporal dependencies, with the bulk of existing\n",
            "studies in the time domain. An instance of this is STGCN [54],\n",
            "which introduces temporal gated convolution that integrates\n",
            "1-D convolution with gated linear units (GLU) to facilitate\n",
            "tractable model training. Other representative examples include\n",
            "MTGNN [51] and MTGODE [23]. An alternative strand ofmethodologies, including StemGNN [50] and TGC [28], focuses\n",
            "on modeling temporal clues in the frequency domain.\n",
            "Attention Models: Recently, a growing number of method-\n",
            "ologies is turning towards attention mechanisms, e.g., the self-\n",
            "attention in transformer [124] , to embed temporal correlations.\n",
            "For instance, GMAN [70] attentively aggregates historical in-\n",
            "formation by considering both spatial and temporal features.\n",
            "ST-GRAT [125] mirrors the transformer’s architecture to embed\n",
            "historical observations in conjunction with its proposed spatial\n",
            "attention mechanism. Recent strides such as STEP [61] similarly\n",
            "employ transformer layers to model the temporal dependencies\n",
            "within each univariate time series.\n",
            "Hybrid Models: Hybrid models also ﬁnd application in\n",
            "modeling inter-temporal dependencies. For example, AST-\n",
            "GCN [86] and DSTAGNN [88] concurrently employ temporal\n",
            "attention and convolution in learning temporal correlations.\n",
            "STGNN* [87] amalgamates both GRU and transformer to cap-\n",
            "ture local and global temporal dependencies. In the frequency\n",
            "domain, the nonlinear variant of TGC [28] captures temporal\n",
            "relations through the combination of spectral attention and con-\n",
            "volution models.\n",
            "Complete discussion of modeling inter-temporal dependen-\n",
            "cies is in Appendix A.2 , available online.\n",
            "C. Forecasting Architectural Fusion\n",
            "Given the spatial and temporal modules discussed, denoted\n",
            "as S PATIAL(·)and T EMPORAL (·), four categories of neural ar-\n",
            "chitectural fusion have been identiﬁed as effective means to\n",
            "capture spatial-temporal dependencies within time series data:\n",
            "(1)discrete factorized ,( 2 ) discrete coupled ,( 3 ) continuous\n",
            "factorized , and (4) continuous coupled . In discrete factorized\n",
            "models, spatial and temporal dependencies are usually learned\n",
            "and processed independently. Discrete coupled models, on the\n",
            "other hand, explicitly or implicitly incorporate spatial and tem-\n",
            "poral modules into a singular process when modeling spatial-\n",
            "temporal dependencies. Different from discrete models, some\n",
            "methods abstract the underlying modeling processes with neural\n",
            "differential equations, which we categorize as continuous mod-\n",
            "els. Speciﬁcally, continuous factorized models involve distinct\n",
            "processes, either partially or entirely continuous (e.g., [74]), to\n",
            "model spatial and temporal dependencies. In contrast, contin-\n",
            "uous coupled models employ a single continuous process to\n",
            "accomplish this task, such as [23] and[75].\n",
            "Authorized licensed use limited to: Mapua University. Downloaded on April 02,2025 at 08:19:48 UTC from IEEE Xplore.  Restrictions apply. 10476 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 46, NO. 12, DECEMBER 2024\n",
            "Discrete Architectures: Numerous existing GNN-based time\n",
            "series forecasting methods are models processing discrete data.\n",
            "For instance, factorized approaches like STGCN [54] employ\n",
            "a sandwich structure of graph and temporal gated convolution\n",
            "layers as its fundamental building block. Subsequent works,\n",
            "such as DGCNN [126] and HGCN [127] , retain this model\n",
            "architecture while introducing enhancements such as dynamic\n",
            "graph structure estimation and hierarchical graph generation.\n",
            "In the realm of discrete coupled models, early works such as\n",
            "DCRNN [68] and Cirstea et al. [94] straightforwardly incor-\n",
            "porate graph diffusion or attention models into recurrent units.\n",
            "Subsequent works, such as MRA-BGCN [69] and RGSL [72],\n",
            "are based on similar concepts but with varying implementations.\n",
            "There are also some studies integrating spatial and temporal\n",
            "convolution or attention operations into a single module. An\n",
            "example is GMAN [70], which proposes a building block that\n",
            "integrates the spatial and temporal attention mechanisms in a\n",
            "gated manner.\n",
            "Continuous Architectures: To date, only a handful of methods\n",
            "fall into this category. For factorized methods, STGODE [74]\n",
            "proposes to depict the graph propagation as a continuous process\n",
            "with a neural ordinary differential equation (NODE) [128] .\n",
            "This approach allows for the effective characterization of long-\n",
            "range spatial-temporal dependencies in conjunction with dilated\n",
            "convolutions along the time axis. For coupled methods, MT-\n",
            "GODE [23] generalizes both spatial and temporal modeling\n",
            "processes found in most related works into a single uniﬁed\n",
            "process that integrates two NODEs. STG-NCDE [75] shares\n",
            "a similar idea but operates under the framework of neural con-\n",
            "trolled differential equations (NCDEs) [129] .\n",
            "Refer to Appendix A.3 , available online, for more details about\n",
            "the architecture fusion in GNNs for time series forecasting.\n",
            "V . GNN S FOR TIMESERIES ANOMALY DETECTION\n",
            "Time series anomaly detection identiﬁes data observations\n",
            "that deviate from the nominal data-generating process [134] .\n",
            "Anomalies, deﬁned as such deviations, contrast with normal\n",
            "data. Terms like novelty and outlier are often used interchange-\n",
            "ably with anomaly [135] . Deviations can be single observa-\n",
            "tions (points) or a series of observations (subsequences) [136] .\n",
            "However, unlike normal time series data, anomalies are hard\n",
            "to characterize because they are rare, making them difﬁcult to\n",
            "collect and label, and it is usually impossible to establish all\n",
            "potential anomalies, limiting supervised learning. Therefore, un-\n",
            "supervised detection techniques are widely explored as practical\n",
            "solutions.\n",
            "Traditionally, distance-based [137] and distributional tech-\n",
            "niques [138] have been widely used for detecting irregular-\n",
            "ities in time series data. Recently, deep learning has driven\n",
            "signiﬁcant advancements, particularly with recurrent models\n",
            "employing reconstruction [139] and forecasting [140] strategies.\n",
            "These models use forecast and reconstruction errors to measure\n",
            "discrepancies between expected and actual signals, as a model\n",
            "trained on normal data is more likely to fail with anomalous\n",
            "data. However, recurrent models [141] often lack explicit mod-\n",
            "eling of pairwise interdependence among variables, limiting\n",
            "their effectiveness in detecting complex anomalies [35],[142] .Recently, GNNs have shown promise in capturing temporal and\n",
            "spatial dependencies among variables, addressing this gap [36],\n",
            "[56].\n",
            "A. General Approach to Anomaly Detection\n",
            "Treating anomaly detection as an unsupervised task relies on\n",
            "models to learn a general concept of what normality is for a\n",
            "given dataset [143] ,[144] . To achieve this, deep learning ar-\n",
            "chitectures deploy a bifurcated modular framework, constituted\n",
            "by a backbone module and a scoring module [145] . First, a\n",
            "backbone model, B ACKBONE (·), is trained to ﬁt given training\n",
            "data, assumed to be nominal, or to contain very few anomalies.\n",
            "Then, a scoring module, S CORER(X,ˆX), produces a score used\n",
            "to identify the presence of anomalies by comparing the output\n",
            "ˆX=BACKBONE (X)of the backbone module with the observed\n",
            "time series data X. The score is intended as a measure of\n",
            "the discrepancy between the expected signals under normal\n",
            "and anomalous circumstances. Furthermore, it is also important\n",
            "for a model to diagnose anomaly events by pinpointing the\n",
            "responsible variables. Consequently, a scoring function typically\n",
            "computes the discrepancy for each individual channel ﬁrst,\n",
            "before consolidating these discrepancies across all channels into\n",
            "a single anomaly value.\n",
            "To provide a simple illustration of the entire process, the\n",
            "backbone can be a GNN forecaster that makes a one-step-ahead\n",
            "forecast for the scorer. The scorer then computes the anomaly\n",
            "score as the sum of the absolute forecast error for each channel\n",
            "variable, represented as/summationtextN\n",
            "i|xi\n",
            "t−ˆxi\n",
            "t|acrossNchannel vari-\n",
            "ables. Since the ﬁnal score is computed based on the summation\n",
            "of channel errors, an operator can determine the root cause\n",
            "variables by computing the contribution of each variable to the\n",
            "summed error.\n",
            "Advancements in the anomaly detection and diagnosis ﬁeld\n",
            "have led to the proposal of more comprehensive backbone and\n",
            "scoring modules [136] ,[145] , primarily driven by the adoption\n",
            "of GNN methodologies [35],[36],[56].\n",
            "B. Discrepancy Frameworks for Anomaly Detection\n",
            "Most of anomaly detection models follow the same backbone-\n",
            "scorer architecture. However, the way the backbone module is\n",
            "trained to learn data structure from nominal data and the im-\n",
            "plementation of the scoring module differentiate these methods\n",
            "into three categories: (1) reconstruction ,( 2 ) forecast , and (3) re-\n",
            "lational discrepancy frameworks. A summary of representative\n",
            "works is provided in Table III.\n",
            "Reconstruction Discrepancy: Reconstruction discrepancy\n",
            "frameworks operate on the assumption that reconstruction er-\n",
            "ror is low during normal periods and high during anomalies.\n",
            "They are designed to replicate their inputs as outputs, sim-\n",
            "ilar to autoencoders [146] . The backbone is expected to ef-\n",
            "fectively model and reconstruct the training data distribution\n",
            "but not out-of-sample data. To achieve this, these frameworks\n",
            "often include constraints and regularization, such as enforcing\n",
            "a low-dimensional embedded code [147] or using variational\n",
            "objectives [148] . Once the data structure is learned, the model\n",
            "should approximate the input well during normal periods but\n",
            "Authorized licensed use limited to: Mapua University. Downloaded on April 02,2025 at 08:19:48 UTC from IEEE Xplore.  Restrictions apply. JIN et al.: SURVEY ON GRAPH NEURAL NETWORKS FOR TIME SERIES: FORECASTING, CLASSIFICATION, 10477\n",
            "struggle during anomalies. The S CORER(·)then computes a\n",
            "discrepancy score from the reconstructed outputs to identify\n",
            "anomalous events. Although deep reconstruction models gen-\n",
            "erally follow these principles for detecting anomalies, a key dis-\n",
            "tinction between GNNs and other architectural types rests in the\n",
            "backbone reconstructor, B ACKBONE (·), which is characterized\n",
            "by its STGNN implementation. For example, MTAD-GAT [35]\n",
            "and LSTM-V AE [139] both use a variational objective [148] ,\n",
            "but MTAD-GAT employs a graph attention network as a spatial-\n",
            "temporal encoder to learn inter-variable and inter-temporal de-\n",
            "pendencies. Other similar works include VGCRN [79] and\n",
            "FuSAGNet [56]. Another research direction in this category\n",
            "of methods focused on graph-level embeddings to represent\n",
            "the input graph data as vectors to enable the application of\n",
            "well-established and sophisticated detection methods designed\n",
            "for multivariate time series [58],[149] .\n",
            "Forecast Discrepancy: Forecast discrepancy frameworks rely\n",
            "on the assumption that forecast error should be low during\n",
            "normal periods, but high during anomalous periods. Here, the\n",
            "backbone module is substituted with a GNN forecaster that is\n",
            "trained to predict a one-step-ahead forecast. During deployment,\n",
            "the forecaster makes a one-step-ahead prediction, and the fore-\n",
            "casts are given to the scorer. The scorer compares the forecasts\n",
            "against the real observed signals to compute discrepancies such\n",
            "as absolute error [36] or mean-squared error [89]. Importantly, it\n",
            "is generally assumed that a forecasting-based model will exhibit\n",
            "erratic behavior during anomaly periods when the input data\n",
            "deviates from the normal patterns, resulting in a signiﬁcant\n",
            "forecasting discrepancy. GDN [36] is a representative work,\n",
            "consisting of a graph structure module that learns the underlying\n",
            "topology and a graph attention network that encodes input series\n",
            "representations for one-step-ahead forecasts. Then, its scorer\n",
            "computes the forecast discrepancy as the maximum absolute\n",
            "forecast error among the channel variables to indicate whether an\n",
            "anomaly event has occurred. Other similar approaches include\n",
            "GTA [89] and CST-GL [62].\n",
            "While forecast discrepancy frameworks can be similar to\n",
            "reconstruction discrepancy frameworks, they rely on funda-\n",
            "mentally different principles and implementation strategies.\n",
            "Reconstruction discrepancy frameworks project current input\n",
            "onto a latent space, attempting to reconstruct it and identifying\n",
            "anomalies when reconstruction fails. This method uses current\n",
            "input data during both training and inference, focusing on the\n",
            "model’s ability to reconstruct training data but not out-of-sample\n",
            "data. Conversely, forecast discrepancy frameworks use histor-\n",
            "ical data to predict current values and identify anomalies by\n",
            "comparing predictions with actual observations. Reconstruction\n",
            "focuses on replicating current inputs, while forecasting em-\n",
            "phasizes predicting future data points and detecting anomalies\n",
            "through prediction errors. Advanced forecast discrepancy-based\n",
            "methods, such as GST-Pro [150] , can even predict anomalies\n",
            "without using actual observations, allowing anomaly prediction\n",
            "at future timestamps.\n",
            "Relational Discrepancy: Relational discrepancy frameworks\n",
            "rely on the assumption that the relationship between variables\n",
            "should exhibit signiﬁcant shifts from normal to anomalous peri-\n",
            "ods. The logical evolution of using STGNN involves leveragingits capability to learn graph structures for both anomaly detection\n",
            "and diagnosis. In this context, the backbone serves as a graph\n",
            "learning module that constructs the hidden evolving relationship\n",
            "between variables. The scorer, on the other hand, is a func-\n",
            "tion that evaluates changes in these relationships and assigns\n",
            "an anomaly or discrepancy score accordingly. GReLeN [90]\n",
            "pioneered the use of learned dynamic graphs to detect anomalies\n",
            "through relational discrepancies. Its reconstruction module dy-\n",
            "namically constructs graph structures based on input time series\n",
            "data at each time point. These structures are then used by a scorer\n",
            "to compute changes in the in-degree and out-degree values of\n",
            "channel nodes. In contrast, DyGraphAD employs a forecasting\n",
            "approach [82]. It divides a multivariate series into subsequences,\n",
            "converts these into dynamically evolving graphs, and trains the\n",
            "network to predict one-step-ahead graph structures. The scorer\n",
            "in DyGraphAD computes the forecast error in the graph structure\n",
            "as the relational discrepancy for anomaly detection.\n",
            "Hybrid and Other Discrepancies: Different discrepancy-\n",
            "based frameworks offer unique advantages for detecting var-\n",
            "ious types of anomalies. As demonstrated in GDN [36],t h e\n",
            "relational discrepancy framework can uncover spatial anoma-\n",
            "lies that are concealed within the relational patterns between\n",
            "different channels. In contrast, forecast discrepancy frame-\n",
            "works excel at identifying temporal anomalies like sudden\n",
            "spikes or seasonal inconsistencies. A comprehensive solution\n",
            "would leverage the strengths of STGNNs by combining mul-\n",
            "tiple discrepancy measures for anomaly detection. For exam-\n",
            "ple, MTAD-GAT [35] uses both reconstruction and forecast\n",
            "discrepancies, while DyGraphAD [82] combines forecast\n",
            "and relational discrepancies. Additionally, incorporating prior\n",
            "knowledge about anomalous behaviors can enhance detection.\n",
            "For instance, GraphSAD [133] creates pseudo labels for six dis-\n",
            "tinct anomaly types on training data, transforming unsupervised\n",
            "anomaly detection into a standard classiﬁcation task, with class\n",
            "discrepancy as the anomaly indicator.\n",
            "Our detailed discussion about the GNNs for time series\n",
            "anomaly detection can be found in Appendix B , available online.\n",
            "VI. GNN S FOR TIMESERIES CLASSIFICATION\n",
            "Time series classiﬁcation seeks to assign a categorical label\n",
            "to a given time series based on its underlying patterns or char-\n",
            "acteristics. As outlined in a recent survey [154] , early literature\n",
            "primarily focused on distance-based approaches [155] ,[156]\n",
            "and ensembling methods [157] ,[158] . However, despite their\n",
            "strong performance, both strategies struggle with scalability for\n",
            "high-dimensional or large datasets [159] ,[160] . To address these\n",
            "limitations, researchers are exploring deep learning techniques\n",
            "to enhance the performance and scalability of time series classi-\n",
            "ﬁcation. A comprehensive discussion can be found in the latest\n",
            "survey by Foumani et al. [154] , though it does not cover the\n",
            "application of GNNs in this ﬁeld. By transforming time series\n",
            "data into graph representations, one can leverage the powerful\n",
            "capabilities of GNNs to capture both local and global patterns.\n",
            "Furthermore, GNNs are capable of mapping the intricate re-\n",
            "lationships among different time series data samples within a\n",
            "particular dataset. In the following discussion, we provide a fresh\n",
            "Authorized licensed use limited to: Mapua University. Downloaded on April 02,2025 at 08:19:48 UTC from IEEE Xplore.  Restrictions apply. 10478 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 46, NO. 12, DECEMBER 2024\n",
            "GNN perspective on the univariate and multivariate time series\n",
            "classiﬁcation problem. A compilation of representative works is\n",
            "presented in Table IV\n",
            "A. Univariate Time Series Classiﬁcation\n",
            "Time series classiﬁcation inherently differs from other time\n",
            "series analyses by focusing on discerning patterns that dis-\n",
            "tinguish samples based on class labels, rather than capturing\n",
            "patterns within the data. Unlike forecasting future points or de-\n",
            "tecting real-time anomalies, it aims to identify divergent patterns\n",
            "across series . We explore two novel graph-based approaches\n",
            "for univariate time series classiﬁcation: Series-as-Graph and\n",
            "Series-as-Node .\n",
            "Series-as-Graph: This approach transforms a univariate time\n",
            "series into a graph to identify unique patterns that enable accurate\n",
            "classiﬁcation using a GNN. First, each series is broken down into\n",
            "subsequences as nodes, and the nodes are connected with edges\n",
            "illustrating their relationships. Following this, a GNN is applied\n",
            "to make graph classiﬁcation. This procedure is represented in\n",
            "the upper block of Fig. 4(d). Series-as-Graph was ﬁrst pre-\n",
            "sented in Time2Graph [161] , which was later developed further\n",
            "with the incorporation of GNN, as Time2Graph+ [96].T h e\n",
            "Time2Graph+ modeling process can be described as a two-step\n",
            "process: ﬁrst, a time series is transformed into a shapelet graph,\n",
            "and second, a GNN is utilized to model the relations between\n",
            "shapelets along with a graph pooling operation to derive the\n",
            "global representation of the time series. This representation is\n",
            "then fed into a classiﬁer to assign class labels to the time series.\n",
            "Similarly, EC-GCN [152] constructs graphs from encrypted\n",
            "trafﬁc for classiﬁcation. The Series-as-Graph approach can also\n",
            "be extended to multivariate time-series classiﬁcation task [153] .\n",
            "While we use Series-as-Graph to describe the formulation of a\n",
            "time series classiﬁcation task as a graph classiﬁcation task, this\n",
            "should not be confused with Series2Graph [162] , which is for\n",
            "anomaly detection tasks.\n",
            "Series-as-Node: As capturing differentiating class patterns\n",
            "across different series data samples is important, leveraging\n",
            "relationships across the different series data samples in a given\n",
            "dataset can be beneﬁcial for classifying a time series. To achieve\n",
            "this, one can take the Series-as-Node approach, where each\n",
            "series sample is seen as a separate node. These series nodes are\n",
            "connected with edges that represent the relationships between\n",
            "them, creating a large graph that provides a complete view\n",
            "of the entire dataset. This procedure is depicted in the lower\n",
            "block of Fig. 4(d), which essentially formulates a time series\n",
            "classiﬁcation task as a node classiﬁcation task. Series-as-Node\n",
            "was originally presented in SimTSC [63]. In this work, series\n",
            "nodes are connected using edges, which are deﬁned by their\n",
            "pairwise DTW distance, to construct a graph. LB-SimTSC [97]\n",
            "further extends on SimTSC to improve the DTW preprocess-\n",
            "ing efﬁciency by employing the widely-used lower bound for\n",
            "DTW [163] . This allows for a time complexity of O(L)rather\n",
            "thanO(L2), dramatically reducing computation time.\n",
            "More details about GNN-based univariate time series classi-\n",
            "ﬁcation are in Appendix C.1 , available online.B. Multivariate Time Series Classiﬁcation\n",
            "In essence, multivariate time series classiﬁcation main-\n",
            "tains fundamental similarities with its univariate counterpart.\n",
            "However, it introduces an additional layer of complexity: the\n",
            "necessity to capture intricate inter-variable dependencies. For\n",
            "example, given the interconnectedness of brain regions, analyz-\n",
            "ing a single node in isolation may not fully capture the com-\n",
            "prehensive neural dynamics [164] . By modeling inter-variable\n",
            "dependencies, we can understand the relationships between\n",
            "different nodes, thereby offering a more holistic view of brain\n",
            "activity. This facilitates the differentiation of intricate patterns\n",
            "that can classify patients with and without speciﬁc neurological\n",
            "conditions. Since the relationships between the variables, or\n",
            "inter-variable dependencies, can be naturally thought of as a\n",
            "network graph, GNNs are ideally suitable as illustrated before\n",
            "in Section IV. The primary aim here is to effectively distill the\n",
            "complexity of high-dimensional series data into a more compre-\n",
            "hensible, yet equally expressive, representation that enables dif-\n",
            "ferentiation of time series into their representative classes [33],\n",
            "[64],[151] . More discussion is in Appendix C.2 , available online.\n",
            "VII. GNN S FOR TIMESERIES IMPUTATION\n",
            "Time series imputation, a crucial task in numerous real-world\n",
            "applications, involves estimating missing values within one or\n",
            "more data point sequences. Traditional time series imputation\n",
            "approaches have relied on statistical methodologies, such as\n",
            "mean imputation, spline interpolation [171] , and regression\n",
            "models [172] . However, these methods often struggle to capture\n",
            "complex temporal dependencies and non-linear relationships\n",
            "within the data. While some deep neural network-based works,\n",
            "such as [173] and[174] , have mitigated these limitations, they do\n",
            "not explicitly consider inter-variable dependencies. The recent\n",
            "emergence of GNNs offers new possibilities for time series\n",
            "imputation by better capturing intricate spatial and temporal\n",
            "dependencies. From a task perspective, GNN-based time series\n",
            "imputation can be broadly categorized into two types: in-sample\n",
            "imputation andout-of-sample imputation . The former involves\n",
            "ﬁlling in missing values within the given time series data, while\n",
            "the latter predicts missing values in disjoint sequences [37].\n",
            "From a methodological perspective, GNNs for time series im-\n",
            "putation can be categorized into deterministic andprobabilistic\n",
            "approaches. Deterministic imputation provides a single best esti-\n",
            "mate for missing values, while probabilistic imputation accounts\n",
            "for uncertainty by offering a distribution of possible values. In\n",
            "Table V, we summarize most of the related works on GNN\n",
            "for time series imputation to date, offering a comprehensive\n",
            "overview of the ﬁeld and its current state of development.\n",
            "A. In-Sample Imputation\n",
            "The majority of existing GNN-based methods primarily fo-\n",
            "cus on in-sample time series data imputation. For instance,\n",
            "GACN [65] proposes to model spatial-temporal dependencies\n",
            "in time series data by interleaving GAT [120] and temporal\n",
            "convolution layers in its encoder. It then imputes the missing\n",
            "Authorized licensed use limited to: Mapua University. Downloaded on April 02,2025 at 08:19:48 UTC from IEEE Xplore.  Restrictions apply. JIN et al.: SURVEY ON GRAPH NEURAL NETWORKS FOR TIME SERIES: FORECASTING, CLASSIFICATION, 10479\n",
            "TABLE V\n",
            "SUMMARY OF GRAPH NEURAL NETWORKS FOR TIMESERIES IMPUTATION\n",
            "data by combining GAT and temporal deconvolution layers that\n",
            "map latent states back to original feature spaces. GRIN [37]\n",
            "introduces the graph recurrent imputation network, where each\n",
            "unidirectional module consists of one spatial-temporal encoder\n",
            "and two different imputation executors. The spatial-temporal\n",
            "encoder adopted in this work combines MPNN [115] and\n",
            "GRU [118] . After generating the latent time series represen-\n",
            "tations, the ﬁrst-stage imputation ﬁlls missing values with one-\n",
            "step-ahead predicted values, which are then reﬁned by a ﬁnal\n",
            "one-layer MPNN before passing to the second-stage imputa-\n",
            "tion for further processing. Similar works using bidirectional\n",
            "recurrent architectures include AGRN [166] , DGCRIN [167] ,\n",
            "GARNN [168] , and MDGCN [81], where the main differences\n",
            "lie in intermediate processes. Recently, a few research studies\n",
            "have explored probabilistic in-sample time series imputation,\n",
            "such as PriSTI [38] and[170] , where the imputation has been\n",
            "regarded as a generation task. Refer to Appendix D.1 for more\n",
            "details, available online.\n",
            "B. Out-of-Sample Imputation\n",
            "To date, only a few GNN-based methods fall into this category.\n",
            "Among these works, IGNNK [73] proposes an inductive GNN\n",
            "kriging model to recover signals for unobserved time series,\n",
            "such as a new variable in a multivariate time series. In IGNNK,\n",
            "the training process involves masked subgraph sampling and\n",
            "signal reconstruction with the diffusion graph convolution net-\n",
            "work presented in [68]. Another similar work is SATCN [66],\n",
            "and the primary difference between these two works lies in\n",
            "the underlying GNN architectures. INCREASE [169] ,o nt h e\n",
            "other hand, further considers heterogeneous spatial and diverse\n",
            "temporal relations among the locations, lifting the performance\n",
            "of inductive spatio-temporal kriging. Notably, GRIN [37] can\n",
            "handle both in-sample and out-of-sample imputations, as well\n",
            "as[165] . More discussion is in Appendix D.2 , available online.\n",
            "VIII. P RACTICAL APPLICATIONS AND RESOURCES\n",
            "Graph neural networks have been applied to a broad range\n",
            "of disciplines related to time series analysis. We categorize the\n",
            "mainstream applications of GNN4TS into seven areas: smart\n",
            "transportation, on-demand services, environment & sustainable\n",
            "energy, internet-of-things, physical systems, healthcare, andfraud detection. Three prominent areas in this section are dis-\n",
            "cussed. A detailed expansion can be found in Appendix E , avail-\n",
            "able online. In addition, we summarize the common benchmark\n",
            "datasets and the open-sourced implementation of representative\n",
            "models in Appendix F , available online.\n",
            "Smart Transportation: The domain of transportation has been\n",
            "signiﬁcantly transformed with the advent of GNNs, with typical\n",
            "applications spanning from trafﬁc prediction to ﬂight delay\n",
            "prediction. For example, by leveraging advanced algorithms\n",
            "and data analytics related to spatial-temporal GNNs, trafﬁc\n",
            "conditions can be accurately predicted [175] ,[176] , thereby\n",
            "facilitating efﬁcient route planning and congestion management.\n",
            "Another important application is trafﬁc data imputation, which\n",
            "is crucial for maintaining the integrity of trafﬁc databases and\n",
            "ensuring the accuracy of trafﬁc analysis and prediction mod-\n",
            "els[85],[167] . There is also related research on autonomous\n",
            "driving [177] and ﬂight delay prediction [178] ,[179] .T h e\n",
            "use of advanced technologies like GNNs in these applications\n",
            "underscores their transformative impact on smart transporta-\n",
            "tion, emphasizing its critical role in evolving transportation\n",
            "systems.\n",
            "Environment & Sustainable Energy: In this sector, GNNs have\n",
            "been instrumental in wind speed and power prediction, capturing\n",
            "the complex spatial-temporal dynamics of wind patterns to\n",
            "provide accurate predictions that aid in the efﬁcient management\n",
            "of wind energy resources [180] ,[181] . Similarly, in solar energy,\n",
            "GNNs have been used for solar irradiance and photovoltaic (PV)\n",
            "power prediction, modeling the intricate relationships between\n",
            "various factors inﬂuencing solar energy generation to provide\n",
            "accurate predictions [182] ,[183] . Furthermore, GNNs have been\n",
            "employed for air pollution prediction [184] and weather fore-\n",
            "casting [185] that are crucial for various services in agriculture,\n",
            "energy, and transportation.\n",
            "Physical Systems: Systems of interacting objects are found\n",
            "in numerous scientiﬁc ﬁelds, including the simulation of n-\n",
            "body systems [186] , particle physics [187] , modeling of human\n",
            "motion dynamics [188] , and prediction of molecular dynam-\n",
            "ics[189] . In graph-based deep learning, the objects are repre-\n",
            "sented as nodes of a graph and GNNs have proven effective\n",
            "in modeling their complex interactions. Despite the challenges\n",
            "posed by physical constraints, promising performance has been\n",
            "achieved thanks to the incorporation of inductive biases from\n",
            "known physical laws [190] ,[191] and architectures that maintain\n",
            "Authorized licensed use limited to: Mapua University. Downloaded on April 02,2025 at 08:19:48 UTC from IEEE Xplore.  Restrictions apply. 10480 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 46, NO. 12, DECEMBER 2024\n",
            "the symmetries of the underlying system, such as equivariance\n",
            "to rotations and translations [189] ,[192] ,[193] .\n",
            "Other Applications: The application of GNNs for time se-\n",
            "ries analysis has also been extended to various other ﬁelds,\n",
            "such as ﬁnance [194] ,[195] , on-demand services [41], fraud\n",
            "detection [196] , manufacturing [197] , and recommender sys-\n",
            "tems [198] . As research in this area continues to evolve, it is\n",
            "anticipated that the application of GNN4TS will continue to\n",
            "expand, opening up new possibilities for data-driven decision\n",
            "making and system optimization.\n",
            "IX. F UTURE DIRECTIONS\n",
            "The future of GNNs for time series analysis holds immense\n",
            "promise, driven by several key directions and challenges that\n",
            "need to be addressed to unlock their full potential. More detailed\n",
            "discussion is in Appendix G , available online.\n",
            "Pre-Training, Transfer Learning, and Large Models: Pre-\n",
            "training, transfer learning, and large models are emerging as\n",
            "potent strategies to bolster the performance of GNNs in time\n",
            "series analysis [61],[199] , especially when data is sparse or\n",
            "diverse. These techniques hinge on utilizing learned representa-\n",
            "tions from one or more domains to enhance performance in other\n",
            "related domains [3]. Notable examples include Panagopoulos et\n",
            "al.’s meta-learning for COVID-19 prediction in data-limited set-\n",
            "tings [200] , and Shao et al.’s pre-training framework for spatial-\n",
            "temporal GNNs [61]. The exploration of GNN pre-training\n",
            "and transferability for time series tasks is growing, especially\n",
            "with the advent of generative AI and large models capable of\n",
            "addressing diverse tasks [201] . Challenges include limited time\n",
            "series data for large-scale pre-training, ensuring wide coverage\n",
            "and transferability, and designing strategies to capture complex\n",
            "spatial-temporal dependencies [202] .\n",
            "Robustness: Robustness of GNNs refers to their ability to\n",
            "handle data perturbations and distribution shifts, especially those\n",
            "engineered by adversaries [203] . This is crucial for time series\n",
            "from dynamic systems, as operational failures can compromise\n",
            "the entire system’s integrity [204] . For example, inadequate\n",
            "handling of noise or data corruption in smart city applications\n",
            "can disrupt trafﬁc management, and in healthcare, it can cause\n",
            "missed critical treatment periods. While GNNs perform well in\n",
            "many applications, enhancing their robustness and developing\n",
            "effective failure management strategies are essential.\n",
            "Privacy Enhancing: As GNNs become integral to various\n",
            "sectors, privacy protection is increasingly important. GNNs’\n",
            "capability to learn and reconstruct relationships within complex\n",
            "systems necessitates safeguarding the privacy of both individual\n",
            "entities (nodes) and their relationships (edges) in time series\n",
            "data. The interpretability of GNNs, while useful for identifying\n",
            "vulnerabilities, can also expose sensitive information [205] .\n",
            "Therefore, maintaining robust privacy defenses while capitaliz-\n",
            "ing on the beneﬁts of GNNs for time series analysis requires\n",
            "a delicate balance, one that calls for constant vigilance and\n",
            "continual innovation.\n",
            "Scalability: While GNNs are effective in analyzing complex\n",
            "time series data, their adaptation to vast time-dependent data\n",
            "volumes presents challenges like memory constraints duringcomputations. Traditional GNNs use sampling strategies such\n",
            "as node-wise [206] , layer-wise [207] , and graph-wise [208] to\n",
            "mitigate these issues, but preserving temporal dependencies is\n",
            "complex. Enhancing scalability for real-time GNN applications,\n",
            "especially on edge devices with limited computing power, is\n",
            "crucial. This intersection of scalability and efﬁcient inference is a\n",
            "signiﬁcant research area with potential for major advancements.\n",
            "X. C ONCLUSION\n",
            "This comprehensive survey bridges the knowledge gap in\n",
            "graph neural networks for time series analysis (GNN4TS) by\n",
            "reviewing recent advancements and offering a uniﬁed taxon-\n",
            "omy to categorize existing works. It covers a wide range of\n",
            "analytical tasks, including forecasting, classiﬁcation, anomaly\n",
            "detection, and imputation, providing a detailed understanding\n",
            "of current progress. We also delve into the intricacies of spa-\n",
            "tial and temporal dependencies modeling and overall model\n",
            "architecture, offering a ﬁne-grained classiﬁcation of individual\n",
            "studies. Highlighting the expanding applications of GNN4TS,\n",
            "we demonstrate its versatility and potential for future growth.\n",
            "This survey serves as a valuable resource for practitioners and\n",
            "experts, proposing future research directions to guide and inspire\n",
            "further work in GNN4TS.\n",
            "REFERENCES\n",
            "[1] Q. Wen, L. Yang, T. Zhou, and L. Sun, “Robust time series analysis and\n",
            "applications: An industrial perspective,” in Proc. ACM SIGKDD Conf.\n",
            "Knowl. Discov. Data Mining , 2022, pp. 4836–4837.\n",
            "[2] P. Esling and C. Agon, “Time-series data mining,” ACM Comput. Surv. ,\n",
            "vol. 45, no. 1, pp. 1–34, 2012.\n",
            "[3] K. Zhang et al., “Self-supervised learning for time series analysis: Tax-\n",
            "onomy, progress, and prospects,” 2023, arXiv:2306.10125 .\n",
            "[4] B. Lim and S. Zohren, “Time-series forecasting with deep learning:\n",
            "As u r v e y , ” Philos. Trans. Roy. Soc. A , vol. 379, no. 2194, 2021,\n",
            "Art. no. 20200209.\n",
            "[5] H. I. Fawaz, G. Forestier, J. Weber, L. Idoumghar, and P.-A. Muller,\n",
            "“Deep learning for time series classiﬁcation: A review,” Data Mining\n",
            "Knowl. Discov. , vol. 33, no. 4, pp. 917–963, 2019.\n",
            "[6] A. Blázquez-García, A. Conde, U. Mori, and J. A. Lozano, “A review\n",
            "on outlier/anomaly detection in time series data,” ACM Comput. Surv. ,\n",
            "vol. 54, no. 3, pp. 1–33, 2021.\n",
            "[7] C. Fang and C. Wang, “Time series data imputation: A survey on deep\n",
            "learning approaches,” 2020, arXiv: 2011.11347 .\n",
            "[8] J. Zhang, F.-Y . Wang, K. Wang, W.-H. Lin, X. Xu, and C. Chen, “Data-\n",
            "driven intelligent transportation systems: A survey,” IEEE Trans. Intell.\n",
            "Transp. Syst. , vol. 12, no. 4, pp. 1624–1639, Dec. 2011.\n",
            "[9] Y . Zhou, Z. Ding, Q. Wen, and Y . Wang, “Robust load forecasting towards\n",
            "adversarial attacks via Bayesian learning,” IEEE Trans. Power Syst. ,\n",
            "vol. 38, no. 2, pp. 1445–1459, Mar. 2023.\n",
            "[10] A. A. Cook, G. Mısırlı, and Z. Fan, “Anomaly detection for IoT\n",
            "time-series data: A survey,” IEEE Internet Things J. , vol. 7, no. 7,\n",
            "pp. 6481–6494, Jul. 2020.\n",
            "[11] S. Wang, J. Cao, and S. Y . Philip, “Deep learning for spatio-temporal\n",
            "data mining: A survey,” IEEE Trans. Knowl. Data Eng. , vol. 34, no. 8,\n",
            "pp. 3681–3700, Aug. 2022.\n",
            "[12] J. Ye, J. Zhao, K. Ye, and C. Xu, “How to build a graph-based deep\n",
            "learning architecture in trafﬁc domain: A survey,” IEEE Trans. Intell.\n",
            "Transp. Syst. , vol. 23, no. 5, pp. 3904–3924, May 2022.\n",
            "[13] W. Jiang and J. Luo, “Graph neural network for trafﬁc forecasting: A\n",
            "survey,” Expert Syst. Appl. , vol. 207, 2022, Art. no. 117921.\n",
            "[14] K.-H. N. Bui, J. Cho, and H. Yi, “Spatial-temporal graph neural network\n",
            "for trafﬁc forecasting: An overview and open research issues,” Appl.\n",
            "Intell. , vol. 52, no. 3, pp. 2763–2774, 2022.\n",
            "[15] G. Jin, Y . Liang, Y . Fang, J. Huang, J. Zhang, and Y . Zheng, “Spatio-\n",
            "temporal graph neural networks for predictive learning in urban comput-\n",
            "ing: A survey,” 2023, arXiv:2303.14483 .\n",
            "Authorized licensed use limited to: Mapua University. Downloaded on April 02,2025 at 08:19:48 UTC from IEEE Xplore.  Restrictions apply. JIN et al.: SURVEY ON GRAPH NEURAL NETWORKS FOR TIME SERIES: FORECASTING, CLASSIFICATION, 10481\n",
            "[16] Z. A. Sahili and M. Awad, “Spatio-temporal graph neural networks: A\n",
            "survey,” 2023, arXiv:2301.10569 .\n",
            "[17] S. Rahmani, A. Baghbani, N. Bouguila, and Z. Patterson, “Graph neural\n",
            "networks for intelligent transportation systems: A survey,” IEEE Trans.\n",
            "Intell. Transp. Syst. , vol. 24, no. 8, pp. 8846–8885, Aug. 2023.\n",
            "[18] A. Cini, I. Marisca, D. Zambon, and C. Alippi, “Taming local effects in\n",
            "graph-based spatiotemporal forecasting,” in Proc. Int. Conf. Neural Inf.\n",
            "Process. Syst. , 2023, Art. no. 2417.\n",
            "[19] L.-J. Cao and F. E. H. Tay, “Support vector machine with adaptive\n",
            "parameters in ﬁnancial time series forecasting,” IEEE Trans. Neural Netw.\n",
            "Learn. Syst. , vol. 14, no. 6, pp. 1506–1518, Nov. 2003.\n",
            "[20] Y . Xia and J. Chen, “Trafﬁc ﬂow forecasting method based on gradi-\n",
            "ent boosting decision tree,” in Proc. 5th Int. Conf. Front. Manuf. Sci.\n",
            "Measuring Technol. , Atlantis Press, 2017, pp. 413–416.\n",
            "[21] B. Biller and B. L. Nelson, “Modeling and generating multivariate time-\n",
            "series input processes using a vector autoregressive technique,” ACM\n",
            "Trans. Model. Comput. Simul. , vol. 13, no. 3, pp. 211–237, 2003.\n",
            "[22] G. E. Box and D. A. Pierce, “Distribution of residual autocorrela-\n",
            "tions in autoregressive-integrated moving average time series mod-\n",
            "els,” J. Amer. Statist. Assoc. , vol. 65, no. 332, pp. 1509–1526,\n",
            "1970.\n",
            "[23] M. Jin, Y . Zheng, Y .-F. Li, S. Chen, B. Yang, and S. Pan, “Multivariate\n",
            "time series forecasting with dynamic graph neural ODEs,” IEEE Trans.\n",
            "Knowl. Data Eng. , vol. 35, no. 9, pp. 9168–9180, Sep. 2023.\n",
            "[24] B. Zhao, H. Lu, S. Chen, J. Liu, and D. Wu, “Convolutional neural\n",
            "networks for time series classiﬁcation,” J. Syst. Eng. Electron. , vol. 28,\n",
            "no. 1, pp. 162–169, 2017.\n",
            "[25] A. Borovykh, S. Bohte, and C. W. Oosterlee, “Conditional time se-\n",
            "ries forecasting with convolutional neural networks,” 2017, arXiv:\n",
            "1703.04691 .\n",
            "[26] J. T. Connor, R. D. Martin, and L. E. Atlas, “Recurrent neural networks\n",
            "and robust time series prediction,” IEEE Trans. Neural Netw. Learn. Syst. ,\n",
            "vol. 5, no. 2, pp. 240–254, Mar. 1994.\n",
            "[27] Q. Wen et al., “Transformers in time series: A survey,” in Proc. Int. Joint\n",
            "Conf. Artif. Intell. , 2023, pp. 6778–6786.\n",
            "[28] M. Jin et al., “How expressive are spectral-temporal graph neural net-\n",
            "works for time series forecasting?,” 2023, arXiv:2305.06587 .\n",
            "[29] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and S. Y . Philip, “A\n",
            "comprehensive survey on graph neural networks,” IEEE Trans. Neural\n",
            "Netw. Learn. Syst. , vol. 32, no. 1, pp. 4–24, Jan. 2021.\n",
            "[30] M. Yang, M. Zhou, M. Kalander, Z. Huang, and I. King, “Discrete-time\n",
            "temporal network embedding via implicit hierarchical learning in hyper-\n",
            "bolic space,” in Proc. ACM SIGKDD Conf. Knowl. Discov. Data Mining ,\n",
            "2021, pp. 1975–1985.\n",
            "[31] M. Yang, M. Zhou, H. Xiong, and I. King, “Hyperbolic temporal net-\n",
            "work embedding,” IEEE Trans. Knowl. Data Eng. , vol. 35, no. 11,\n",
            "pp. 11489–11502, Nov. 2023.\n",
            "[32] H. Y . Koh, A. T. N. Nguyen, S. Pan, L. T. May, and G. I. Webb,\n",
            "“Physicochemical graph neural network for learning protein–ligand in-\n",
            "teraction ﬁngerprints from sequence data,” Nature Mach. Intell. ,v o l .6 ,\n",
            "pp. 673–687, 2024.\n",
            "[33] X. Zhang, M. Zeman, T. Tsiligkaridis, and M. Zitnik, “Graph-guided\n",
            "network for irregularly sampled multivariate time series,” in Proc. Int.\n",
            "Conf. Learn. Representations , 2022.\n",
            "[34] Z. Wang, T. Jiang, Z. Xu, J. Gao, and J. Zhang, “Irregularly sampled\n",
            "multivariate time series classiﬁcation: A graph learning approach,” IEEE\n",
            "Intell. Syst. , vol. 38, no. 3, pp. 3–11, May/Jun. 2023.\n",
            "[35] H. Zhao et al., “Multivariate time-series anomaly detection via graph\n",
            "attention network,” in Proc. IEEE Int. Conf. Data Mining , 2020,\n",
            "pp. 841–850.\n",
            "[36] A. Deng and B. Hooi, “Graph neural network-based anomaly detection\n",
            "in multivariate time series,” in Proc. AAAI Conf. Artif. Intell. , 2021,\n",
            "pp. 4027–4035.\n",
            "[37] A. Cini, I. Marisca, and C. Alippi, “Filling the G_ap_s: Multivariate time\n",
            "series imputation by graph neural networks,” in Proc. Int. Conf. Learn.\n",
            "Representations , 2022.\n",
            "[38] M. Liu, H. Huang, H. Feng, L. Sun, B. Du, and Y . Fu, “PriSTI: A\n",
            "conditional diffusion framework for spatiotemporal imputation,” in Proc.\n",
            "IEEE Int. Conf. Data Eng. , 2023, pp. 1927–1939.\n",
            "[39] M. Jin, Y .-F. Li, and S. Pan, “Neural temporal walks: Motif-aware\n",
            "representation learning on continuous-time dynamic graphs,” in Proc.\n",
            "Int. Conf. Neural Inf. Process. Syst. , 2022, Art. no. 1445.\n",
            "[40] Y . Liu et al., “Graph self-supervised learning: A survey,” IEEE Trans.\n",
            "Knowl. Data Eng. , vol. 35, no. 6, pp. 5879–5900, Jun. 2023.[41] X. Geng et al., “Spatiotemporal multi-graph convolution network for\n",
            "ride-hailing demand forecasting,” in Proc. AAAI Conf. Artif. Intell. , 2019,\n",
            "pp. 3656–3663.\n",
            "[42] S. He and K. G. Shin, “Towards ﬁne-grained ﬂow forecasting: A graph\n",
            "attention approach for bike sharing systems,” in Proc. Web Conf. , 2020,\n",
            "pp. 88–98.\n",
            "[43] X. Zhang, R. Cao, Z. Zhang, and Y . Xia, “Crowd ﬂow forecasting with\n",
            "multi-graph neural networks,” in Proc. Int. Joint Conf. Neural Netw. ,\n",
            "2020, pp. 1–7.\n",
            "[44] M. Li and Z. Zhu, “Spatial-temporal fusion graph neural networks\n",
            "for trafﬁc ﬂow forecasting,” in Proc. AAAI Conf. Artif. Intell. , 2021,\n",
            "pp. 4189–4196.\n",
            "[45] S. Yi and V . Pavlovic, “Sparse Granger causality graphs for human\n",
            "action classiﬁcation,” in Proc. Int. Conf. Pattern Recognit. , 2012,\n",
            "pp. 3374–3377.\n",
            "[46] Y . Wang, Z. Duan, Y . Huang, H. Xu, J. Feng, and A. Ren, “MTHetGNN: A\n",
            "heterogeneous graph embedding framework for multivariate time series\n",
            "forecasting,” Pattern Recognit. Lett. , vol. 153, pp. 151–158, 2022.\n",
            "[47] D. Grattarola, D. Zambon, L. Livi, and C. Alippi, “Change detection\n",
            "in graph streams by learning graph embeddings on constant-curvature\n",
            "manifolds,” IEEE Trans. Neural Netw. Learn. Syst. , vol. 31, no. 6,\n",
            "pp. 1856–1869, Jun. 2020.\n",
            "[48] Z. Wu, S. Pan, G. Long, J. Jiang, and C. Zhang, “Graph WaveNet for deep\n",
            "spatial-temporal graph modeling,” in Proc. Int. Joint Conf. Artif. Intell. ,\n",
            "2019, pp. 1907–1913.\n",
            "[49] L. Bai, L. Yao, C. Li, X. Wang, and C. Wang, “Adaptive graph convolu-\n",
            "tional recurrent network for trafﬁc forecasting,” in Proc. Int. Conf. Neural\n",
            "Inf. Process. Syst. , 2020, Art. no. 1494.\n",
            "[50] D. Cao et al., “Spectral temporal graph neural network for multivariate\n",
            "time-series forecasting,” in Proc. Int. Conf. Neural Inf. Process. Syst. ,\n",
            "2020, Art. no. 1491.\n",
            "[51] Z. Wu, S. Pan, G. Long, J. Jiang, X. Chang, and C. Zhang, “Connecting\n",
            "the dots: Multivariate time series forecasting with graph neural net-\n",
            "works,” in Proc. ACM SIGKDD Conf. Knowl. Discov. Data Mining , 2020,\n",
            "pp. 753–763.\n",
            "[52] C. Shang, J. Chen, and J. Bi, “Discrete graph structure learning for fore-\n",
            "casting multiple time series,” in Proc. Int. Conf. Learn. Representations ,\n",
            "2021.\n",
            "[53] A. Cini, D. Zambon, and C. Alippi, “Sparse graph learning from spa-\n",
            "tiotemporal time series,” J. Mach. Learn. Res. , vol. 24, no. 242, pp. 1–36,\n",
            "2023.\n",
            "[54] B. Yu, H. Yin, and Z. Zhu, “Spatio-temporal graph convolutional net-\n",
            "works: A deep learning framework for trafﬁc forecasting,” in Proc. Int.\n",
            "Joint Conf. Artif. Intell. , 2018, pp. 3634–3640.\n",
            "[55] H. Wen et al., “DiffSTG: Probabilistic spatio-temporal graph forecasting\n",
            "with denoising diffusion models,” in Proc. 31st ACM Int. Conf. Adv.\n",
            "Geographic Inf. Syst. , 2023, Art. no. 60.\n",
            "[56] S. Han and S. S. Woo, “Learning sparse latent graph representations for\n",
            "anomaly detection in multivariate time series,” in Proc. ACM SIGKDD\n",
            "Conf. Knowl. Discov. Data Mining , 2022, pp. 2977–2986.\n",
            "[57] E. Dai and J. Chen, “Graph-augmented normalizing ﬂows for anomaly\n",
            "detection of multiple time series,” in Proc. Int. Conf. Learn. Representa-\n",
            "tions , 2022.\n",
            "[58] D. Zambon, C. Alippi, and L. Livi, “Concept drift and anomaly detection\n",
            "in graph streams,” IEEE Trans. Neural Netw. Learn. Syst. , vol. 29, no. 11,\n",
            "pp. 5592–5605, Nov. 2018.\n",
            "[59] D. Zambon and C. Alippi, “Where and how to improve graph-based\n",
            "spatio-temporal predictors,” 2023, arXiv:2302.01701 .\n",
            "[60] Y . Liu et al., “Multivariate time-series forecasting with temporal poly-\n",
            "nomial graph neural networks,” in Proc. Int. Conf. Neural Inf. Process.\n",
            "Syst. , 2022, Art. no. 1411.\n",
            "[61] Z. Shao, Z. Zhang, F. Wang, and Y . Xu, “Pre-training enhanced spatial-\n",
            "temporal graph neural network for multivariate time series forecast-\n",
            "ing,” in Proc. ACM SIGKDD Conf. Knowl. Discov. Data Mining , 2022,\n",
            "pp. 1567–1577.\n",
            "[62] Y . Zheng et al., “Correlation-aware spatial-temporal graph learning\n",
            "for multivariate time-series anomaly detection,” IEEE Trans. Neural\n",
            "Netw. Learn. Syst. , early access, Nov. 14, 2023, doi: 10.1109/TNNLS.\n",
            "2023.3325667 .\n",
            "[63] D. Zha, K.-H. Lai, K. Zhou, and X. Hu, “Towards similarity-aware\n",
            "time-series classiﬁcation,” in Proc. SIAM Int. Conf. Data Mining , 2022,\n",
            "pp. 199–207.\n",
            "[64] H. Liu et al., “TodyNet: Temporal dynamic graph neural network for\n",
            "multivariate time series classiﬁcation,” 2023, arXiv:2304.05078 .\n",
            "Authorized licensed use limited to: Mapua University. Downloaded on April 02,2025 at 08:19:48 UTC from IEEE Xplore.  Restrictions apply. 10482 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 46, NO. 12, DECEMBER 2024\n",
            "[65] Y . Ye, S. Zhang, and J. J. Yu, “Spatial-temporal trafﬁc data imputation via\n",
            "graph attention convolutional network,” in Proc. Int. Conf. Artif. Neural\n",
            "Netw. , 2021, pp. 241–252.\n",
            "[66] Y . Wu, D. Zhuang, M. Lei, A. Labbe, and L. Sun, “Spatial ag-\n",
            "gregation and temporal convolution networks for real-time Kriging,”\n",
            "2021, arXiv:2109.12144 .\n",
            "[67] I. Marisca, A. Cini, and C. Alippi, “Learning to reconstruct missing data\n",
            "from spatiotemporal graphs with sparse observations,” in Proc. Int. Conf.\n",
            "Neural Inf. Process. Syst. , 2022, Art. no. 2324.\n",
            "[68] Y . Li, R. Yu, C. Shahabi, and Y . Liu, “Diffusion convolutional recurrent\n",
            "neural network: Data-driven trafﬁc forecasting,” in Proc. Int. Conf. Learn.\n",
            "Representations , 2018.\n",
            "[69] W. Chen, L. Chen, Y . Xie, W. Cao, Y . Gao, and X. Feng, “Multi-range at-\n",
            "tentive bicomponent graph convolutional network for trafﬁc forecasting,”\n",
            "inProc. AAAI Conf. Artif. Intell. , 2020, pp. 3529–3536.\n",
            "[70] C. Zheng, X. Fan, C. Wang, and J. Qi, “GMAN: A graph multi-attention\n",
            "network for trafﬁc prediction,” in Proc. AAAI Conf. Artif. Intell. , 2020,\n",
            "pp. 1234–1241.\n",
            "[71] Y . Chen, I. Segovia-Dominguez, and Y . R. Gel, “Z-GCNETs: Time\n",
            "zigzags at graph convolutional networks for time series forecasting,” in\n",
            "Proc. Int. Conf. Mach. Learn. , 2021, pp. 1684–1694.\n",
            "[72] H. Yu et al., “Regularized graph structure learning with semantic knowl-\n",
            "edge for multi-variates time-series forecasting,” in Proc. Int. Joint Conf.\n",
            "Artif. Intell. , 2022, pp. 2362–2368.\n",
            "[73] Y . Wu, D. Zhuang, A. Labbe, and L. Sun, “Inductive graph neural\n",
            "networks for spatiotemporal Kriging,” in Proc. AAAI Conf. Artif. Intell. ,\n",
            "2021, pp. 4478–4485.\n",
            "[74] Z. Fang, Q. Long, G. Song, and K. Xie, “Spatial-temporal graph ODE\n",
            "networks for trafﬁc ﬂow forecasting,” in Proc. ACM SIGKDD Conf.\n",
            "Knowl. Discov. Data Mining , 2021, pp. 364–373.\n",
            "[75] J. Choi, H. Choi, J. Hwang, and N. Park, “Graph neural controlled\n",
            "differential equations for trafﬁc forecasting,” in Proc. AAAI Conf. Artif.\n",
            "Intell. , 2022, pp. 6367–6374.\n",
            "[76] Z. Pan, Y . Liang, W. Wang, Y . Yu, Y . Zheng, and J. Zhang, “Urban trafﬁc\n",
            "prediction from spatio-temporal data using deep meta learning,” in Proc.\n",
            "ACM SIGKDD Conf. Knowl. Discov. Data Mining , 2019, pp. 1720–1730.\n",
            "[77] A. Cini, I. Marisca, F. Bianchi, and C. Alippi, “Scalable spatiotempo-\n",
            "ral graph neural networks,” in Proc. AAAI Conf. Artif. Intell. , 2023,\n",
            "pp. 7218–7226.\n",
            "[78] W. Hu, Y . Yang, Z. Cheng, C. Yang, and X. Ren, “Time-series event\n",
            "prediction with evolutionary state graph,” in Proc. ACM Int. Conf. Web\n",
            "Search Data Mining , 2021, pp. 580–588.\n",
            "[79] W. Chen, L. Tian, B. Chen, L. Dai, Z. Duan, and M. Zhou, “Deep\n",
            "variational graph convolutional recurrent network for multivariate time\n",
            "series anomaly detection,” in Proc. Int. Conf. Mach. Learn. , 2022,\n",
            "pp. 3621–3633.\n",
            "[80] L. Zhou, Q. Zeng, and B. Li, “Hybrid anomaly detection via multihead\n",
            "dynamic graph attention networks for multivariate time series,” IEEE\n",
            "Access , vol. 10, pp. 40 967–40 978, 2022.\n",
            "[81] Y . Liang, Z. Zhao, and L. Sun, “Memory-augmented dynamic graph\n",
            "convolution networks for trafﬁc data imputation with diverse missing\n",
            "patterns,” Transp. Res. Part C Emerg. , vol. 143, 2022, Art. no. 103826.\n",
            "[82] K. Chen, M. Feng, and T. S. Wirjanto, “Multivariate time series anomaly\n",
            "detection via dynamic graph forecasting,” 2023, arXiv:2302.02051 .\n",
            "[83] C. Yu, X. Ma, J. Ren, H. Zhao, and S. Yi, “Spatio-temporal graph\n",
            "transformer networks for pedestrian trajectory prediction,” in Proc. Eur.\n",
            "Conf. Comput. Vis. , Springer, 2020, pp. 507–523.\n",
            "[84] Y . Wu, M. Gu, L. Wang, Y . Lin, F. Wang, and H. Yang, “Event2Graph:\n",
            "Event-driven bipartite graph for multivariate time-series anomaly detec-\n",
            "tion,” 2021, arXiv:2108.06783 .\n",
            "[85] X. Wu, M. Xu, J. Fang, and X. Wu, “A multi-attention tensor completion\n",
            "network for spatiotemporal trafﬁc data imputation,” IEEE Internet Things\n",
            "J., vol. 9, no. 20, pp. 20 203–20 213, Oct. 2022.\n",
            "[86] S. Guo, Y . Lin, N. Feng, C. Song, and H. Wan, “Attention based spatial-\n",
            "temporal graph convolutional networks for trafﬁc ﬂow forecasting,” in\n",
            "Proc. AAAI Conf. Artif. Intell. , 2019, pp. 922–929.\n",
            "[87] X. Wang et al., “Trafﬁc ﬂow prediction via spatial temporal graph neural\n",
            "network,” in Proc. Web Conf. , 2020, pp. 1082–1092.\n",
            "[88] S. Lan, Y . Ma, W. Huang, W. Wang, H. Yang, and P. Li, “DSTAGNN:\n",
            "Dynamic spatial-temporal aware graph neural network for trafﬁc ﬂow\n",
            "forecasting,” in Proc. Int. Conf. Mach. Learn. , 2022, pp. 11 906–11 917.\n",
            "[89] Z. Chen, D. Chen, X. Zhang, Z. Yuan, and X. Cheng, “Learning graph\n",
            "structures with transformer for multivariate time-series anomaly detec-\n",
            "tion in IoT,” IEEE Internet Things J. , vol. 9, no. 12, pp. 9179–9189,\n",
            "Jun. 2022.[90] W. Zhang, C. Zhang, and F. Tsung, “GRELEN: Multivariate time series\n",
            "anomaly detection from the perspective of graph relational learning,” in\n",
            "Proc. Int. Joint Conf. Artif. Intell. , 2022, pp. 2390–2397.\n",
            "[91] Y . Xia et al., “Deciphering spatio-temporal graph forecasting: A causal\n",
            "lens and treatment,” in Proc. Int. Conf. Neural Inf. Process. Syst. , 2023,\n",
            "Art. no. 1611.\n",
            "[92] C. Song, Y . Lin, S. Guo, and H. Wan, “Spatial-temporal synchronous\n",
            "graph convolutional networks: A new framework for spatial-temporal\n",
            "network data forecasting,” in Proc. AAAI Conf. Artif. Intell. , 2020,\n",
            "pp. 914–921.\n",
            "[93] Y . Cui et al., “METRO: A generic graph neural network framework for\n",
            "multivariate time series forecasting,” in Proc. VLDB Endowment , vol. 15,\n",
            "no. 2, pp. 224–236, 2021.\n",
            "[94] R.-G. Cirstea, B. Yang, and C. Guo, “Graph attention recurrent neural\n",
            "networks for correlated time series forecasting,” in Proc. 5th SIGKDD\n",
            "Workshop Mining Learn. Time Ser. , 2019, pp. 1–6.\n",
            "[95] Q. Zhang, J. Chang, G. Meng, S. Xiang, and C. Pan, “Spatio-temporal\n",
            "graph structure learning for trafﬁc forecasting,” in Proc. AAAI Conf. Artif.\n",
            "Intell. , 2020, pp. 1177–1185.\n",
            "[96] Z. Cheng et al., “Time2Graph+: Bridging time series and graph represen-\n",
            "tation learning via multiple attentions,” IEEE Trans. Knowl. Data Eng. ,\n",
            "vol. 35, no. 2, pp. 2078–2090, Feb. 2023.\n",
            "[97] W. Xi, A. Jain, L. Zhang, and J. Lin, “LB-SimTSC: An efﬁcient\n",
            "similarity-aware graph neural network for semi-supervised time series\n",
            "classiﬁcation,” 2023, arXiv:2301.04838 .\n",
            "[98] D. I. Shuman, S. K. Narang, P. Frossard, A. Ortega, and P. Vandergheynst,\n",
            "“The emerging ﬁeld of signal processing on graphs: Extending high-\n",
            "dimensional data analysis to networks and other irregular domains,” IEEE\n",
            "Signal Process. Mag. , vol. 30, no. 3, pp. 83–98, May 2013.\n",
            "[99] A. Sandryhaila and J. M. Moura, “Discrete signal processing on graphs,”\n",
            "IEEE Trans. Image Process. , vol. 61, no. 7, pp. 1644–1656, Apr.\n",
            "2013.\n",
            "[100] C. Ying et al., “Do transformers really perform badly for graph repre-\n",
            "sentation?,” in Proc. Int. Conf. Neural Inf. Process. Syst. , 2021, pp. 28\n",
            "877–28 888.\n",
            "[101] C. Cai, T. S. Hy, R. Yu, and Y . Wang, “On the connection between\n",
            "MPNN and graph transformer,” in Proc. Int. Conf. Mach. Learn. , 2023,\n",
            "pp. 3408–3430.\n",
            "[102] R. Wan, S. Mei, J. Wang, M. Liu, and F. Yang, “Multivariate temporal\n",
            "convolutional network: A deep neural networks approach for multivariate\n",
            "time series forecasting,” Electronics , vol. 8, no. 8, 2019, Art. no. 876.\n",
            "[103] J. Gao and B. Ribeiro, “On the equivalence between temporal and static\n",
            "equivariant graph representations,” in Proc. Int. Conf. Mach. Learn. ,\n",
            "2022, pp. 7052–7076.\n",
            "[104] D. Zambon, D. Grattarola, C. Alippi, and L. Livi, “Autoregressive models\n",
            "for sequences of graphs,” in Proc. Int. Joint Conf. Neural Netw. , 2019,\n",
            "pp. 1–8.\n",
            "[105] R. Huang, C. Huang, Y . Liu, G. Dai, and W. Kong, “LSGCN: Long\n",
            "short-term trafﬁc prediction with graph convolutional networks,” in Proc.\n",
            "Int. Joint Conf. Artif. Intell. , 2020, pp. 2355–2361.\n",
            "[106] B. Paassen, D. Grattarola, D. Zambon, C. Alippi, and B. E. Hammer,\n",
            "“Graph edit networks,” in Proc. Int. Conf. Learn. Representations , 2021.\n",
            "[107] J. Chauhan, A. Raghuveer, R. Saket, J. Nandy, and B. Ravindran,\n",
            "“Multi-variate time series forecasting on variable subsets,” in Proc. ACM\n",
            "SIGKDD Conf. Knowl. Discov. Data Mining , 2022, pp. 76–86.\n",
            "[108] X. Rao, H. Wang, L. Zhang, J. Li, S. Shang, and P. Han, “FOGS:\n",
            "First-order gradient supervision with learning-based graph for trafﬁc ﬂow\n",
            "forecasting,” in Proc. Int. Joint Conf. Artif. Intell. , 2022, pp. 3926–3932.\n",
            "[109] A. Cini, D. Mandic, and C. Alippi, “Graph-based time series clustering\n",
            "for end-to-end hierarchical forecasting,” 2023, arXiv:2305.19183 .\n",
            "[110] Z. Li, L. Xia, Y . Xu, and C. Huang, “GPT-ST: Generative pre-training of\n",
            "spatio-temporal graph neural networks,” in Proc. Int. Conf. Neural Inf.\n",
            "Process. Syst. , 2024, Art. no. 3077.\n",
            "[111] G. Lai, W. Chang, Y . Yang, and H. Liu, “Modeling long- and short-term\n",
            "temporal patterns with deep neural networks,” in Proc. 41st Int. ACM\n",
            "SIGIR Conf. Res. Develop. Inf. Retrieval , 2018, pp. 95–104.\n",
            "[112] S.-Y . Shih, F.-K. Sun, and H.-Y . Lee, “Temporal pattern attention\n",
            "for multivariate time series forecasting,” Mach. Learn. , vol. 108,\n",
            "pp. 1421–1441, 2019.\n",
            "[113] M. Defferrard, X. Bresson, and P. Vandergheynst, “Convolutional neural\n",
            "networks on graphs with fast localized spectral ﬁltering,” in Proc. Int.\n",
            "Conf. Neural Inf. Process. Syst. , 2016, pp. 3837–3845.\n",
            "[114] C. Lea, M. D. Flynn, R. Vidal, A. Reiter, and G. D. Hager, “Temporal\n",
            "convolutional networks for action segmentation and detection,” in Proc.\n",
            "IEEE Conf. Comput. Vis. Pattern Recognit. , 2017, pp. 1003–1012.\n",
            "Authorized licensed use limited to: Mapua University. Downloaded on April 02,2025 at 08:19:48 UTC from IEEE Xplore.  Restrictions apply. JIN et al.: SURVEY ON GRAPH NEURAL NETWORKS FOR TIME SERIES: FORECASTING, CLASSIFICATION, 10483\n",
            "[115] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl,\n",
            "“Neural message passing for quantum chemistry,” in Proc. Int. Conf.\n",
            "Mach. Learn. , 2017, pp. 1263–1272.\n",
            "[116] J. Klicpera, S. Weißenberger, and S. Günnemann, “Diffusion improves\n",
            "graph learning,” in Proc. Int. Conf. Neural Inf. Process. Syst. , 2019, pp. 13\n",
            "333–13 345.\n",
            "[117] Y . Zheng, H. Zhang, V . Lee, Y . Zheng, X. Wang, and S. Pan, “Finding\n",
            "the missing-half: Graph complementary learning for homophily-prone\n",
            "and heterophily-prone graphs,” in Proc. Int. Conf. Mach. Learn. , 2023,\n",
            "Art. no. 1788.\n",
            "[118] J. Chung, C. Gulcehre, K. Cho, and Y . Bengio, “Empirical evaluation of\n",
            "gated recurrent neural networks on sequence modeling,” in Proc. NeurIPS\n",
            "Workshop Deep Learn. , 2014.\n",
            "[119] T. N. Kipf and M. Welling, “Semi-supervised classiﬁcation with graph\n",
            "convolutional networks,” in Proc. Int. Conf. Learn. Representations ,\n",
            "2017.\n",
            "[120] P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Liò, and Y .\n",
            "Bengio, “Graph attention networks,” in Proc. Int. Conf. Learn. Repre-\n",
            "sentations , 2018.\n",
            "[121] Y . Feng, H. You, Z. Zhang, R. Ji, and Y . Gao, “Hypergraph neural\n",
            "networks,” in Proc. AAAI Conf. Artif. Intell. , 2019, pp. 3558–3565.\n",
            "[122] A. Feng and L. Tassiulas, “Adaptive graph spatial-temporal transformer\n",
            "network for trafﬁc forecasting,” in Proc. ACM Int. Conf. Inf. Knowl.\n",
            "Manage. , 2022, pp. 3933–3937.\n",
            "[123] C. Wang, K. Zhang, H. Wang, and B. Chen, “Auto-STGCN: Au-\n",
            "tonomous spatial-temporal graph convolutional network search,” ACM\n",
            "Trans. Knowl. Discov. Data , vol. 17, no. 5, pp. 1–21, 2023.\n",
            "[124] A. Vaswani et al., “Attention is all you need,” in Proc. Int. Conf. Neural\n",
            "Inf. Process. Syst. , 2017, pp. 5998–6008.\n",
            "[125] C. Park et al., “ST-GRAT: A novel spatio-temporal graph attention\n",
            "networks for accurately forecasting dynamically changing road speed,”\n",
            "inProc. ACM Int. Conf. Inf. Knowl. Manage. , 2020, pp. 1215–1224.\n",
            "[126] Z. Diao, X. Wang, D. Zhang, Y . Liu, K. Xie, and S. He, “Dynamic spatial-\n",
            "temporal graph convolutional neural networks for trafﬁc forecasting,” in\n",
            "Proc. AAAI Conf. Artif. Intell. , 2019, pp. 890–897.\n",
            "[127] K. Guo, Y . Hu, Y . Sun, S. Qian, J. Gao, and B. Yin, “Hierarchical graph\n",
            "convolution network for trafﬁc forecasting,” in Proc. AAAI Conf. Artif.\n",
            "Intell. , 2021, pp. 151–159.\n",
            "[128] T. Q. Chen, Y . Rubanova, J. Bettencourt, and D. Duvenaud, “Neural\n",
            "ordinary differential equations,” in Proc. Int. Conf. Neural Inf. Process.\n",
            "Syst. , 2018, pp. 6572–6583.\n",
            "[129] P. Kidger, J. Morrill, J. Foster, and T. J. Lyons, “Neural controlled\n",
            "differential equations for irregular time series,” in Proc. Int. Conf. Neural\n",
            "Inf. Process. Syst. , 2020, Art. no. 562.\n",
            "[130] S. Guan, B. Zhao, Z. Dong, M. Gao, and Z. He, “GTAD: Graph and\n",
            "temporal neural network for multivariate time series anomaly detection,”\n",
            "Entropy , vol. 24, no. 6, 2022, Art. no. 759.\n",
            "[131] S. S. Srinivas, R. K. Sarkar, and V . Runkana, “Hypergraph learning based\n",
            "recommender system for anomaly detection, control and optimization,”\n",
            "inProc. IEEE Int. Conf. Big Data , 2022, pp. 1922–1929.\n",
            "[132] D. Zambon, L. Livi, and C. Alippi, “Graph iForest: Isolation of anomalous\n",
            "and outlier graphs,” in Proc. Int. Joint Conf. Neural Netw. , 2022, pp. 1–8.\n",
            "[133] W. Chen, Z. Zhou, Q. Wen, and L. Sun, “Time series subsequence\n",
            "anomaly detection via graph neural networks,” 2023. [Online]. Available:\n",
            "https://openreview.net/forum?id=73U_NlKaNx\n",
            "[134] D. M. Hawkins, Identiﬁcation of Outliers , vol. 11. Berlin, Germany:\n",
            "Springer, 1980.\n",
            "[135] M. A. Pimentel, D. A. Clifton, L. Clifton, and L. Tarassenko, “A review\n",
            "of novelty detection,” Signal Process. , vol. 99, pp. 215–249, 2014.\n",
            "[136] Z. Z. Darban, G. I. Webb, S. Pan, C. C. Aggarwal, and M.\n",
            "Salehi, “Deep learning for time series anomaly detection: A survey,”\n",
            "2022, arXiv:2211.05244 .\n",
            "[137] E. Keogh, J. Lin, and A. Fu, “HOT SAX: Efﬁciently ﬁnding the most\n",
            "unusual time series subsequence,” in Proc. 5th IEEE Int. Conf. Data\n",
            "Mining , 2005, Art. no. 8.\n",
            "[138] K. M. Ting, B.-C. Xu, T. Washio, and Z.-H. Zhou, “Isolation distributional\n",
            "kernel a new tool for point & group anomaly detection,” IEEE Trans.\n",
            "Knowl. Data Eng. , vol. 35, no. 3, pp. 2697–2710, Mar. 2023.\n",
            "[139] D. Park, Y . Hoshi, and C. C. Kemp, “A multimodal anomaly detector for\n",
            "robot-assisted feeding using an LSTM-based variational autoencoder,”\n",
            "IEEE Trans. Robot. Autom. , vol. 3, no. 3, pp. 1544–1551, Jul. 2018.\n",
            "[140] K. Hundman, V . Constantinou, C. Laporte, I. Colwell, and T. Söderström,\n",
            "“Detecting spacecraft anomalies using LSTMs and nonparametric dy-\n",
            "namic thresholding,” in Proc. ACM SIGKDD Conf. Knowl. Discov. Data\n",
            "Mining , 2018, pp. 387–395.[141] Y . Su, Y . Zhao, C. Niu, R. Liu, W. Sun, and D. Pei, “Robust anomaly\n",
            "detection for multivariate time series through stochastic recurrent neural\n",
            "network,” in Proc. ACM SIGKDD Conf. Knowl. Discov. Data Mining ,\n",
            "2019, pp. 2828–2837.\n",
            "[142] J. Xu, H. Wu, J. Wang, and M. Long, “Anomaly transformer: Time\n",
            "series anomaly detection with association discrepancy,” in Proc. Int. Conf.\n",
            "Learn. Representations , 2022.\n",
            "[143] M. Jin, Y . Liu, Y . Zheng, L. Chi, Y .-F. Li, and S. Pan, “ANEMONE:\n",
            "Graph anomaly detection with multi-scale contrastive learning,” in Proc.\n",
            "ACM Int. Conf. Inf. Knowl. Manage. , 2021, pp. 3122–3126.\n",
            "[144] Y . Zheng, M. Jin, Y . Liu, L. Chi, K. T. Phan, and Y .-P. P. Chen, “Generative\n",
            "and contrastive self-supervised learning for graph anomaly detection,”\n",
            "IEEE Trans. Knowl. Data Eng. , vol. 35, no. 12, pp. 12220–12233,\n",
            "Dec. 2023.\n",
            "[145] A. Garg, W. Zhang, J. Samaran, R. Savitha, and C.-S. Foo, “An eval-\n",
            "uation of anomaly detection and diagnosis in multivariate time series,”\n",
            "IEEE Trans. Neural Netw. Learn. Syst. , vol. 33, no. 6, pp. 2508–2517,\n",
            "Jun. 2022.\n",
            "[146] I. Goodfellow, Y . Bengio, and A. Courville, Deep Learning . Cambridge,\n",
            "MA, USA: MIT Press, 2016.\n",
            "[147] M. Ranzato, Y . Boureau, and Y . LeCun, “Sparse feature learning for deep\n",
            "belief networks,” in Proc. Int. Conf. Neural Inf. Process. Syst. , 2007,\n",
            "pp. 1185–1192.\n",
            "[148] D. P. Kingma et al., “An introduction to variational autoencoders,” Found.\n",
            "Trends Mach. Learn. , vol. 12, no. 4, pp. 307–392, 2019.\n",
            "[149] D. Zambon, C. Alippi, and L. Livi, “Change-point methods on a sequence\n",
            "of graphs,” IEEE Trans. Signal Process. , vol. 67, no. 24, pp. 6327–6341,\n",
            "Dec. 2019.\n",
            "[150] Y . Zheng et al., “Graph spatiotemporal process for multivariate time series\n",
            "anomaly detection with missing values,” Inf. Fusion , vol. 106, 2024,\n",
            "Art. no. 102255.\n",
            "[151] Z. Duan et al., “Multivariate time-series classiﬁcation with hierarchical\n",
            "variational graph pooling,” Neural Netw. , vol. 154, pp. 481–490, 2022.\n",
            "[152] Z. Diao et al., “EC-GCN: A encrypted trafﬁc classiﬁcation frame-\n",
            "work based on multi-scale graph convolution networks,” Comput. Netw. ,\n",
            "vol. 224, 2023, Art. no. 109614.\n",
            "[153] R. Younis, A. Hakmeh, and Z. Ahmadi, “MTS2Graph: Interpretable\n",
            "multivariate time series classiﬁcation with temporal evolving graphs,”\n",
            "Pattern Recognit. , vol. 152, 2024, Art. no. 110486.\n",
            "[154] N. M. Foumani, L. Miller, C. W. Tan, G. I. Webb, G. Forestier, and\n",
            "M. Salehi, “Deep learning for time series classiﬁcation and extrinsic\n",
            "regression: A current survey,” 2023, arXiv:2302.02515 .\n",
            "[155] J. Lines and A. Bagnall, “Time series classiﬁcation with ensembles\n",
            "of elastic distance measures,” Data Mining Knowl. Discov. , vol. 29,\n",
            "pp. 565–592, 2015.\n",
            "[156] M. Herrmann and G. I. Webb, “Amercing: An intuitive, elegant and\n",
            "effective constraint for dynamic time warping,” 2021, arXiv:2111.13314 .\n",
            "[157] J. Lines, S. Taylor, and A. Bagnall, “Time series classiﬁcation with\n",
            "HIVE-COTE: The hierarchical vote collective of transformation-based\n",
            "ensembles,” ACM Trans. Knowl. Discov. Data , vol. 12, no. 5, 2018,\n",
            "Art. no. 52.\n",
            "[158] M. Middlehurst, J. Large, M. Flynn, J. Lines, A. Bostrom, and A. Bagnall,\n",
            "“HIVE-COTE 2.0: A new meta ensemble for time series classiﬁcation,”\n",
            "Mach. Learn. , vol. 110, no. 11/12, pp. 3211–3243, 2021.\n",
            "[159] A. Dempster, F. Petitjean, and G. I. Webb, “ROCKET: Exceptionally\n",
            "fast and accurate time series classiﬁcation using random convolutional\n",
            "kernels,” Data Mining Knowl. Discov. , vol. 34, no. 5, pp. 1454–1495,\n",
            "2020.\n",
            "[160] C. W. Tan, A. Dempster, C. Bergmeir, and G. I. Webb, “MultiRocket:\n",
            "Multiple pooling operators and transformations for fast and effective\n",
            "time series classiﬁcation,” Data Mining Knowl. Discov. , vol. 36, no. 5,\n",
            "pp. 1623–1646, 2022.\n",
            "[161] Z. Cheng, Y . Yang, W. Wang, W. Hu, Y . Zhuang, and G. Song,\n",
            "“Time2Graph: Revisiting time series modeling with dynamic shapelets,”\n",
            "inProc. AAAI Conf. Artif. Intell. , 2020, pp. 3617–3624.\n",
            "[162] P. Boniol and T. Palpanas, “Series2Graph: Graph-based subsequence\n",
            "anomaly detection for time series,” in Proc. VLDB Endowment , vol. 13,\n",
            "no. 12, pp. 1821–1834, 2022.\n",
            "[163] E. Keogh and C. A. Ratanamahatana, “Exact indexing of dynamic time\n",
            "warping,” Knowl. Inf. Syst. , vol. 7, pp. 358–386, 2005.\n",
            "[164] S. Tang et al., “Self-supervised graph neural networks for improved\n",
            "electroencephalographic seizure analysis,” in Proc. Int. Conf. Learn.\n",
            "Representations , 2022.\n",
            "[165] A. Roth and T. Liebig, “Forecasting unobserved node states with spatio-\n",
            "temporal graph neural networks,” 2022, arXiv:2211.11596 .\n",
            "Authorized licensed use limited to: Mapua University. Downloaded on April 02,2025 at 08:19:48 UTC from IEEE Xplore.  Restrictions apply. 10484 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 46, NO. 12, DECEMBER 2024\n",
            "[166] Y . Chen, Z. Li, C. Yang, X. Wang, G. Long, and G. Xu, “Adaptive graph\n",
            "recurrent network for multivariate time series imputation,” in Proc. Int.\n",
            "Conf. Neural Inf. Process. , 2023, pp. 64–73.\n",
            "[167] X. Kong, W. Zhou, G. Shen, W. Zhang, N. Liu, and Y . Yang, “Dynamic\n",
            "graph convolutional recurrent imputation network for spatiotemporal\n",
            "trafﬁc missing data,” Knowl.-Based Syst. , vol. 261, 2023, Art. no. 110188.\n",
            "[168] G. Shen, W. Zhou, W. Zhang, N. Liu, Z. Liu, and X. Kong, “Bidirectional\n",
            "spatial-temporal trafﬁc data imputation via graph attention recurrent\n",
            "neural network,” Neurocomputing , vol. 531, pp. 151–162, 2023.\n",
            "[169] C. Zheng, X. Fan, C. Wang, J. Qi, C. Chen, and L. Chen, “INCREASE:\n",
            "Inductive graph representation learning for spatio-temporal Kriging,” in\n",
            "Proc. ACM Web Conf. , 2023, pp. 673–683.\n",
            "[170] T. Yun, H. Jung, and J. Son, “Imputation as inpainting: Diffusion models\n",
            "for spatiotemporal data imputation,” 2023. [Online]. Available: https:\n",
            "//openreview.net/forum?id=QUANtQnx30l\n",
            "[171] S. Moritz and T. Bartz-Beielstein, “imputeTS: Time series missing value\n",
            "imputation in R,” RJ ., vol. 9, no. 1, 2017, Art. no. 207.\n",
            "[172] M. Saad, M. Chaudhary, F. Karray, and V . Gaudet, “Machine learning\n",
            "based approaches for imputation in time series data and their impact\n",
            "on forecasting,” in Proc. IEEE Int. Conf. Syst. Man Cybern. , 2020,\n",
            "pp. 2621–2627.\n",
            "[173] Z. Che, S. Purushotham, K. Cho, D. Sontag, and Y . Liu, “Recurrent\n",
            "neural networks for multivariate time series with missing values,” Sci.\n",
            "Rep., vol. 8, no. 1, 2018, Art. no. 6085.\n",
            "[174] X. Miao, Y . Wu, J. Wang, Y . Gao, X. Mao, and J. Yin, “Generative semi-\n",
            "supervised learning for multivariate time series imputation,” in Proc.\n",
            "AAAI Conf. Artif. Intell. , 2021, pp. 8983–8991.\n",
            "[175] Y . Tang, A. Qu, A. H. Chow, W. H. Lam, S. Wong, and W. Ma, “Domain\n",
            "adversarial spatial-temporal network: A transferable framework for short-\n",
            "term trafﬁc forecasting across cities,” in Proc. ACM Int. Conf. Inf. Knowl.\n",
            "Manage. , 2022, pp. 1905–1915.\n",
            "[176] H. Li et al., “Trafﬁc ﬂow forecasting in the COVID-19: A deep spatial-\n",
            "temporal model based on discrete wavelet transformation,” ACM Trans.\n",
            "Knowl. Discov. Data , vol. 17, no. 5, pp. 1–28, 2023.\n",
            "[177] L. Tang, F. Yan, B. Zou, W. Li, C. Lv, and K. Wang, “Trajectory prediction\n",
            "for autonomous driving based on multiscale spatial-temporal graph,” IET\n",
            "Intell. Transp. Syst. , vol. 17, no. 2, pp. 386–399, 2023.\n",
            "[178] K. Cai, Y . Li, Y .-P. Fang, and Y . Zhu, “A deep learning approach for\n",
            "ﬂight delay prediction through time-evolving graphs,” IEEE Trans. Intell.\n",
            "Transp. Syst. , vol. 23, no. 8, pp. 11 397–11 407, Aug. 2022.\n",
            "[179] Z. Guo et al., “SGDAN—A spatio-temporal graph dual-attention neural\n",
            "network for quantiﬁed ﬂight delay prediction,” Sensors , vol. 20, no. 22,\n",
            "2020, Art. no. 6433.\n",
            "[180] Q. Wu, H. Zheng, X. Guo, and G. Liu, “Promoting wind energy for\n",
            "sustainable development by precise wind speed prediction based on graph\n",
            "neural networks,” Renewable Energy , vol. 199, pp. 977–992, 2022.\n",
            "[181] Y . He, S. Chai, J. Zhao, Y . Sun, and X. Zhang, “A robust spatio-temporal\n",
            "prediction approach for wind power generation based on spectral tempo-\n",
            "ral graph neural network,” IET Renewable Power Gener. , vol. 16, no. 12,\n",
            "pp. 2556–2565, 2022.\n",
            "[182] X. Jiao, X. Li, D. Lin, and W. Xiao, “A graph neural network based deep\n",
            "learning predictor for spatio-temporal group solar irradiance forecasting,”\n",
            "IEEE Trans. Ind. Inform. , vol. 18, no. 9, pp. 6142–6149, Sep. 2022.\n",
            "[183] M. Zhang et al., “Optimal graph structure based short-term solar\n",
            "PV power forecasting method considering surrounding spatio-temporal\n",
            "correlations,” IEEE Trans. Ind Appl. , vol. 59, no. 1, pp. 345–357,\n",
            "Jan./Feb. 2023.\n",
            "[184] V . O. Santos, P. A. C. Rocha, J. Scott, J. Van Griensven Thé, and B.\n",
            "Gharabaghi, “Spatiotemporal air pollution forecasting in Houston-TX:\n",
            "A case study for ozone using deep graph neural networks,” Atmosphere ,\n",
            "vol. 14, no. 2, 2023, Art. no. 308.\n",
            "[185] R. Keisler, “Forecasting global weather with graph neural networks,”\n",
            "2022, arXiv:2202.07575 .\n",
            "[186] P. Battaglia, R. Pascanu, M. Lai, D. J. Rezende, and K. Kavukcuoglu,\n",
            "“Interaction networks for learning about objects, relations and physics,”\n",
            "inProc. Int. Conf. Neural Inf. Process. Syst. , 2016, pp. 4502–4510.\n",
            "[187] G. Shi, D. Zhang, M. Jin, and S. Pan, “Towards complex dynamic physics\n",
            "system simulation with graph neural ODEs,” 2023, arXiv:2305.12334 .\n",
            "[188] Y . Liu, S. Magliacane, M. Koﬁnas, and E. Gavves, “Graph switch-\n",
            "ing dynamical systems,” in Proc. Int. Conf. Mach. Learn. , 2023,\n",
            "pp. 21867–21883.\n",
            "[189] L. Wu, Z. Hou, J. Yuan, Y . Rong, and W. Huang, “Equivariant spatio-\n",
            "temporal attentive graph networks to simulate physical dynamics,” in\n",
            "Proc. 37th Int. Conf. Neural Inf. Process. Syst. , 2023, Art. no. 1965.[190] A. Sanchez-Gonzalez, V . Bapst, K. Cranmer, and P. Battaglia, “Hamilto-\n",
            "nian graph networks with ODE integrators,” 2019, arXiv: 1909.12790 .\n",
            "[191] Y . Liu et al., “SEGNO: Generalizing equivariant graph neural networks\n",
            "with physical inductive biases,” in Proc. Int. Conf. Learn. Representa-\n",
            "tions , 2023.\n",
            "[192] V . G. Satorras, E. Hoogeboom, and M. Welling, “E(n) equivariant graph\n",
            "neural networks,” in Proc. Int. Conf. Mach. Learn. , 2021, pp. 9323–9332.\n",
            "[193] J. Brandstetter, R. Hesselink, E. van der Pol, E. J. Bekkers, and M.\n",
            "Welling, “Geometric and physical quantities improve E(3) equivariant\n",
            "message passing,” in Proc. Int. Conf. Learn. Representations , 2021.\n",
            "[194] J. Wang, S. Zhang, Y . Xiao, and R. Song, “A review on graph neural\n",
            "network methods in ﬁnancial applications,” J. Data Sci. , vol. 20, no. 2,\n",
            "pp. 111–134, 2022.\n",
            "[195] Z. Song, Y . Zhang, and I. King, “Towards fair ﬁnancial services for all: A\n",
            "temporal GNN approach for individual fairness on transaction networks,”\n",
            "inProc. ACM Int. Conf. Inf. Knowl. Manage. , 2023, pp. 2331–2341.\n",
            "[196] N. Noorshams, S. Verma, and A. Hoﬂeitner, “TIES: Temporal inter-\n",
            "action embeddings for enhancing social media integrity at Facebook,”\n",
            "inProc. ACM SIGKDD Conf. Knowl. Discov. Data Mining , 2020,\n",
            "pp. 3128–3135.\n",
            "[197] X.-Y . Yao, G. Chen, M. Pecht, and B. Chen, “A novel graph-based\n",
            "framework for state of health prediction of lithium-ion battery,” J. Energy\n",
            "Storage , vol. 58, 2023, Art. no. 106437.\n",
            "[198] S. Wu, Y . Tang, Y . Zhu, L. Wang, X. Xie, and T. Tan, “Session-based\n",
            "recommendation with graph neural networks,” in Proc. AAAI Conf. Artif.\n",
            "Intell. , 2019, pp. 346–353.\n",
            "[199] X. Wang, D. Wang, L. Chen, and Y . Lin, “Building transportation founda-\n",
            "tion model via generative graph transformer,” 2023, arXiv:2305.14826 .\n",
            "[200] G. Panagopoulos, G. Nikolentzos, and M. Vazirgiannis, “Transfer graph\n",
            "neural networks for pandemic forecasting,” in Proc. AAAI Conf. Artif.\n",
            "Intell. , 2021, pp. 4838–4845.\n",
            "[201] M. Jin et al., “Large models for time series and spatio-temporal data: A\n",
            "survey and outlook,” 2023, arXiv:2310.10196 .\n",
            "[202] W. X. Zhao et al., “A survey of large language models,”\n",
            "2023, arXiv:2303.18223 .\n",
            "[203] H. Zhang, B. Wu, X. Yuan, S. Pan, H. Tong, and J. Pei, “Trustworthy\n",
            "graph neural networks: Aspects, methods and trends,” in Proc. IEEE ,\n",
            "vol. 112, no. 2, pp. 97–139, Feb. 2024.\n",
            "[204] G. Dong et al., “Graph neural networks in IoT: A survey,” ACM Trans.\n",
            "Sensor Netw. , vol. 19, no. 2, pp. 1–50, 2023.\n",
            "[205] J. Xu, M. Xue, and S. Picek, “Explainability-based backdoor attacks\n",
            "against graph neural networks,” in Proc. 3rd ACM Workshop Wireless\n",
            "Secur. Mach. Learn. , 2021, pp. 31–36.\n",
            "[206] W. L. Hamilton, Z. Ying, and J. Leskovec, “Inductive representation\n",
            "learning on large graphs,” in Proc. Int. Conf. Neural Inf. Process. Syst. ,\n",
            "2017, pp. 1024–1034.\n",
            "[207] J. Chen, T. Ma, and C. Xiao, “FastGCN: Fast learning with graph\n",
            "convolutional networks via importance sampling,” in Proc. Int. Conf.\n",
            "Learn. Representations , 2018.\n",
            "[208] W. Chiang, X. Liu, S. Si, Y . Li, S. Bengio, and C. Hsieh, “Cluster-GCN:\n",
            "An efﬁcient algorithm for training deep and large graph convolutional\n",
            "networks,” in Proc. 25th ACM SIGKDD Int. Conf. Knowl. Discov. Data\n",
            "Mining , 2019, pp. 257–266.\n",
            "Ming Jin received the PhD degree from Monash\n",
            "University, Australia, in 2024. He is currently an\n",
            "assistant professor with the School of Information and\n",
            "Communication Technology, Grifﬁth University. He\n",
            "has published more than twenty peer-reviewed papers\n",
            "in top-ranked journals and conferences, including\n",
            "IEEE Transactions on Pattern Analysis and Machine\n",
            "Intelligence ,IEEE Transactions on Knowledge and\n",
            "Data Engineering , NeurIPS, ICLR, ICML, etc. He\n",
            "serves as an associate editor of the Journal of Neuro-\n",
            "computing and regularly contributes as area chair/PC\n",
            "member for major AI conferences. His research interests include time series\n",
            "analysis, graph neural networks, and multi-modal learning.\n",
            "Authorized licensed use limited to: Mapua University. Downloaded on April 02,2025 at 08:19:48 UTC from IEEE Xplore.  Restrictions apply. JIN et al.: SURVEY ON GRAPH NEURAL NETWORKS FOR TIME SERIES: FORECASTING, CLASSIFICATION, 10485\n",
            "Huan Yee Koh received the master of data science\n",
            "and bachelors of commerce degrees from Monash\n",
            "University, Melbourne, Australia, in 2021 and 2018,\n",
            "respectively. He is currently working toward the PhD\n",
            "degree in machine learning with Monash University.\n",
            "His research focuses on graph neural networks, time\n",
            "series analysis, drug discovery, data mining, and ma-\n",
            "chine learning.\n",
            "Qingsong Wen received the PhD degree in electrical\n",
            "and computer engineering from the Georgia Institute\n",
            "of Technology. He is the head of AI Research &\n",
            "Chief Scientist, Squirrel Ai Learning. He has pub-\n",
            "lished more than 100 top-ranked AI conference and\n",
            "journal papers, had multiple Oral/Spotlight Papers\n",
            "at NeurIPS, ICML, and ICLR, had multiple Most\n",
            "Inﬂuential Papers at IJCAI, received multiple IAAI\n",
            "Deployed Application Awards at AAAI, and won\n",
            "First Place of SP Grand Challenge at ICASSP. He\n",
            "organizes workshops on AI for Time Series and AI\n",
            "for education and serves as an associate editor of the Neurocomputing andIEEE\n",
            "Signal Processing Letters , and guest editor of the Applied Energy and IEEE\n",
            "Internet of Things Journal . His research focuses on AI for time series, AI for\n",
            "education, and general machine learning.\n",
            "Daniele Zambon received the PhD degree from the\n",
            "Università della Svizzera italiana. He is a postdoctoral\n",
            "researcher with the Swiss AI Lab IDSIA, Università\n",
            "della Svizzera Italiana (Switzerland). He has been\n",
            "a visiting researcher/intern with the University of\n",
            "Florida (US), the University of Exeter (U.K.), and\n",
            "STMicroelectronics (Italy). He is a member of the\n",
            "IEEE CIS Task Force on Learning for Graphs and\n",
            "has co-organized special sessions and tutorials on\n",
            "deep learning and graph data. His main research\n",
            "interests include graph representation learning, time\n",
            "series analysis, and learning in non-stationary environments.\n",
            "Cesare Alippi (Fellow, IEEE) is a professor with\n",
            "the Università della Svizzera italiana and Politec-\n",
            "nico di Milano, a visiting professor with the Uni-\n",
            "versity of Guangzhou, and an advisory profes-\n",
            "sor with Northwestern Polytechnic, Xi’an. He has\n",
            "authored/coauthored one monograph, seven edited\n",
            "books, and around 200 papers, and holds eight\n",
            "patents. His research focuses on adaptation and learn-\n",
            "ing in non-stationary environments, graph learning,\n",
            "intelligence for embedded systems, IoT, and cyber-\n",
            "physical systems. He is a fellow of the European\n",
            "Laboratory for Learning and Intelligent Systems, serves on the IEEE Compu-\n",
            "tational Intelligence Society (CIS) Administrative Committee, and is a board\n",
            "of governors member of the International Neural Network Society. He has\n",
            "held various roles within IEEE CIS, including vice-president for education\n",
            "and awards committee chair, and has received several awards, including the\n",
            "International Neural Networks Society Gabor Award.\n",
            "Geoffrey I. Webb (Fellow, IEEE) is a professor with\n",
            "the Monash University Department of Data Science\n",
            "and AI, Australia. He was editor in chief of the\n",
            "Leading Data Mining Journal ,Data Mining and\n",
            "Knowledge Discovery , from 2005 to 2014. He has\n",
            "been program committee chair of both the leading\n",
            "data mining conferences, ACM SIGKDD and IEEE\n",
            "ICDM, as well as general chair of ICDM. He is\n",
            "a technical advisor to the startups BigML Inc and\n",
            "FROOMLE. He developed many of the key mecha-\n",
            "nisms of support-conﬁdence association discovery in\n",
            "the 1980s. He pioneered multiple research areas as diverse as black-box user\n",
            "modelling, interactive data analytics, and statistically-sound pattern discovery.\n",
            "His many awards include IEEE ICDM Ten-Year Impact Award (2023) and the\n",
            "inaugural Eureka Prize for Excellence in Data Science.\n",
            "Irwin King (Fellow, IEEE) received the BS degree in\n",
            "computer science from Caltech, and the PhD degree\n",
            "in computer science from the University of Southern\n",
            "California. He is professor with the Department of\n",
            "Computer Science & Engineering, Chinese Univer-\n",
            "sity of Hong Kong. His research interests include\n",
            "machine learning, social computing, AI, web intel-\n",
            "ligence, data mining, and multimedia information\n",
            "processing, with more than 300 technical publica-\n",
            "tions in these areas. He is an associate editor of the\n",
            "Journal of Neural Networks , an ACM distinguished\n",
            "member, and a fellow of the International Neural Network Society (INNS),\n",
            "and Hong Kong Institute of Engineers (HKIE). He has served as president of\n",
            "the International Neural Network Society (INNS) and as general co-chair of\n",
            "several major conferences. Currently, he is the vice-chair of ACM SIGWEB and\n",
            "WebConf Steering Committee. He has received the ACM CIKM 2019 Test of\n",
            "Time Award, the ACM SIGIR 2020 Test of Time Award, and the 2020 APNNS\n",
            "Outstanding Achievement Award for contributions to social computing with\n",
            "machine learning.\n",
            "Shirui Pan (Senior Member, IEEE) received the PhD\n",
            "degree in computer science from the University of\n",
            "Technology Sydney (UTS), Ultimo, NSW, Australia.\n",
            "He is a professor with the School of Information\n",
            "and Communication Technology, Grifﬁth University,\n",
            "Australia. Prior to this, he was a senior lecturer with\n",
            "the Faculty of IT, Monash University. His research\n",
            "interests include data mining and machine learning.\n",
            "To date, he has published more than 100 research\n",
            "papers in top-tier journals and conferences, including\n",
            "IEEE Transactions on Pattern Analysis and Machine\n",
            "Intelligence ,IEEE Transactions on Knowledge and Data Engineering ,IEEE\n",
            "Transactions on Neural Networks and Learning Systems , ICML, NeurIPS, and\n",
            "KDD. His research received the 2024 CIS IEEE TNNLS Oustanding Paper\n",
            "Award and the 2020 IEEE ICDM Best Student Paper Award. He is recognized\n",
            "as one of the AI 2000 AAAI/IJCAI Most Inﬂuential Scholars in Australia. He is\n",
            "an ARC future fellow and a fellow of Queensland Academy of Arts and Sciences\n",
            "(FQA).\n",
            "Authorized licensed use limited to: Mapua University. Downloaded on April 02,2025 at 08:19:48 UTC from IEEE Xplore.  Restrictions apply. \n"
          ]
        }
      ],
      "source": [
        "print(raw_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9OO8_wUSmQg"
      },
      "source": [
        "Connect to database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5ZD5qbNGShBA"
      },
      "outputs": [],
      "source": [
        "cassio.init(token=ASTRA_DB_APPLICATION_TOKEN, database_id=ASTRA_DB_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_KYVEPxSwoN"
      },
      "source": [
        "LangChain embedding and LLM objects for later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzZ2l3dwSz0c",
        "outputId": "b4c7a796-c9b4-45e7-dac2-f87d194b6a2c"
      },
      "outputs": [],
      "source": [
        "llm = OpenAI(openai_api_key=OPENAI_API_KEY)\n",
        "embedding = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1DANSjkS-TI"
      },
      "source": [
        "Create LangChain vector store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "t41m3qZBTAL9"
      },
      "outputs": [],
      "source": [
        "astra_vector_store = Cassandra(\n",
        "    embedding=embedding,\n",
        "    table_name='qa_demo',\n",
        "    session=None,\n",
        "    keyspace=None,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "3gFJU6bfTPNT"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator='\\n',\n",
        "    chunk_size=800,\n",
        "    chunk_overlap=200,\n",
        "    length_function=len,\n",
        ")\n",
        "\n",
        "texts = text_splitter.split_text(raw_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXO3sIH4TutO",
        "outputId": "3dfa54d7-8b6d-44f3-8cf7-6fbcff767297"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['10466 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 46, NO. 12, DECEMBER 2024\\nA Survey on Graph Neural Networks for Time Series:\\nForecasting, Classiﬁcation, Imputation, and\\nAnomaly Detection\\nMing Jin ,H u a nY e eK o h , Qingsong Wen , Daniele Zambon , Cesare Alippi , Fellow, IEEE ,\\nGeoffrey I. Webb , Fellow, IEEE , Irwin King , Fellow, IEEE , and Shirui Pan , Senior Member, IEEE\\n(Survey Paper)\\nAbstract —Time series are the primary data type used to record\\ndynamic system measurements and generated in great volume by\\nboth physical sensors and online processes (virtual sensors). Time\\nseries analytics is therefore crucial to unlocking the wealth of in-\\nformation implicit in available data. With the recent advancements',\n",
              " 'series analytics is therefore crucial to unlocking the wealth of in-\\nformation implicit in available data. With the recent advancements\\nin graph neural networks (GNNs), there has been a surge in GNN-\\nbased approaches for time series analysis. These approaches can\\nexplicitly model inter-temporal and inter-variable relationships,\\nwhich traditional and other deep neural network-based methods\\nstruggle to do. In this survey, we provide a comprehensive re-\\nview of graph neural networks for time series analysis (GNN4TS),\\nencompassing four fundamental dimensions: forecasting, classiﬁ-\\ncation, anomaly detection, and imputation. Our aim is to guide\\ndesigners and practitioners to understand, build applications, and\\nadvance research of GNN4TS. At ﬁrst, we provide a comprehensive',\n",
              " 'cation, anomaly detection, and imputation. Our aim is to guide\\ndesigners and practitioners to understand, build applications, and\\nadvance research of GNN4TS. At ﬁrst, we provide a comprehensive\\ntask-oriented taxonomy of GNN4TS. Then, we present and discuss\\nrepresentative research works and introduce mainstream applica-\\ntions of GNN4TS. A comprehensive discussion of potential future\\nresearch directions completes the survey. This survey, for the ﬁrst\\ntime, brings together a vast array of knowledge on GNN-based time\\nseries research, highlighting foundations, practical applications,\\nand opportunities of graph neural networks for time series analysis.\\nManuscript received 17 December 2023; revised 8 July 2024; accepted\\n8 August 2024. Date of publication 14 August 2024; date of current version',\n",
              " 'Manuscript received 17 December 2023; revised 8 July 2024; accepted\\n8 August 2024. Date of publication 14 August 2024; date of current version\\n5 November 2024. This work was supported in part by the CSIRO– National Sci-\\nence Foundation (US) AI Research Collaboration Program and Swiss National\\nScience Foundation under Grant 204061. The work of Shirui Pan was supported\\nin part by the Australian Research Council (ARC) under Grant FT210100097\\nand Grant DP240101547. Recommended for acceptance by W. Liu. (Ming Jin\\nand Huan Yee Koh contributed equally to this work.) (Corresponding author:\\nShirui Pan.)\\nMing Jin and Shirui Pan are with the School of Information and Communi-\\ncation Technology, Grifﬁth University, Nathan, QLD 4111, Australia (e-mail:\\nmingjinedu@gmail.com; s.pan@grifﬁth.edu.au).',\n",
              " 'Ming Jin and Shirui Pan are with the School of Information and Communi-\\ncation Technology, Grifﬁth University, Nathan, QLD 4111, Australia (e-mail:\\nmingjinedu@gmail.com; s.pan@grifﬁth.edu.au).\\nHuan Yee Koh and Geoffrey I. Webb are with the Department of Data\\nScience and AI, Monash University, Clayton, VIC 3800, Australia (e-mail:\\nhuan.koh@monash.edu; geoff.webb@monash.edu).\\nQingsong Wen is with Squirrel AI Learning, Bellevue, WA 98004 USA (e-\\nmail: qingsongedu@gmail.com).\\nDaniele Zambon is with the Swiss AI Lab IDSIA, Università della Svizzera\\nItaliana, 6900 Lugano, Switzerland (e-mail: daniele.zambon@usi.ch).\\nCesare Alippi is with the Swiss AI Lab IDSIA, Università della Svizzera\\nItaliana, 6900 Lugano, Switzerland, and also with Politecnico di Milano, 20133',\n",
              " 'Cesare Alippi is with the Swiss AI Lab IDSIA, Università della Svizzera\\nItaliana, 6900 Lugano, Switzerland, and also with Politecnico di Milano, 20133\\nMilano, Italy (e-mail: cesare.alippi@usi.ch).\\nIrwin King is with the Department of Computer Science & Engineer-\\ning, Chinese University of Hong Kong, Ma Liu Shui, Hong Kong (e-mail:\\nking@cse.cuhk.edu.hk).\\nThis article has supplementary downloadable material available at\\nhttps://doi.org/10.1109/TPAMI.2024.3443141, provided by the authors.\\nDigital Object Identiﬁer 10.1109/TPAMI.2024.3443141Index Terms —Time series, graph neural networks, deep\\nlearning, forecasting, classiﬁcation, imputation, anomaly detection.\\nI. I NTRODUCTION\\nTHE advent of advanced sensing and data stream processing',\n",
              " 'learning, forecasting, classiﬁcation, imputation, anomaly detection.\\nI. I NTRODUCTION\\nTHE advent of advanced sensing and data stream processing\\ntechnologies has led to an explosion of time series data [1],\\n[2],[3]. The analysis of time series not only provides insights\\ninto past trends but also facilitates a multitude of tasks such\\nas forecasting [4], classiﬁcation [5], anomaly detection [6],\\nand data imputation [7]. This lays the groundwork for time\\nseries modeling paradigms that leverage on historical data to\\nunderstand current and future possibilities. Time series analytics\\nhave become increasingly crucial in various ﬁelds, including but\\nnot limited to cloud computing, transportation, energy, ﬁnance,\\nsocial networks, and the Internet-of-Things [8],[9],[10].',\n",
              " 'have become increasingly crucial in various ﬁelds, including but\\nnot limited to cloud computing, transportation, energy, ﬁnance,\\nsocial networks, and the Internet-of-Things [8],[9],[10].\\nMany time series involve complex interactions across time\\n(such as lags in propagation of effects) and variables (such as\\nthe relationship among the variables representing neighboring\\ntrafﬁc sensors). By treating time points or variables as nodes and\\ntheir relationships as edges, a model structured in the manner\\nof a network or graph can effectively solve the task at hand by\\nexploiting both data and relational information. Indeed, much\\ntime series data is spatial-temporal in nature, with different\\nvariables in the series capturing information about different',\n",
              " 'exploiting both data and relational information. Indeed, much\\ntime series data is spatial-temporal in nature, with different\\nvariables in the series capturing information about different\\nlocations – space – too, meaning it encapsulates not only time in-\\nformation but also spatial relationships [11]. This is particularly\\nevident in scenarios such as urban trafﬁc networks, population\\nmigration, and global weather forecasting. In these instances, a\\nlocalized change, such as a trafﬁc accident at an intersection, an\\nepidemic outbreak in a suburb, or extreme weather in a speciﬁc\\narea, can propagate and inﬂuence neighboring regions. This\\nspatial-temporal characteristic is a common feature of many\\ndynamic systems, including the wind farm in Fig. 1, where',\n",
              " 'area, can propagate and inﬂuence neighboring regions. This\\nspatial-temporal characteristic is a common feature of many\\ndynamic systems, including the wind farm in Fig. 1, where\\nthe underlying time series displays a range of correlations and\\nheterogeneities [15],[18]. Traditional analytic tools, such as\\nsupport vector regression (SVR) [19], gradient boosting decision\\ntree (GBDT) [20], vector autoregressive (V AR) [21], and au-\\ntoregressive integrated moving average (ARIMA) [22], struggle\\nto handle complex time series relations (e.g., nonlinearities and\\ninter-variable relationships), resulting in less accurate prediction\\n0162-8828 © 2024 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.',\n",
              " 'inter-variable relationships), resulting in less accurate prediction\\n0162-8828 © 2024 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\\nSee https://www.ieee.org/publications/rights/index.html for more information.\\nAuthorized licensed use limited to: Mapua University. Downloaded on April 02,2025 at 08:19:48 UTC from IEEE Xplore.  Restrictions apply. JIN et al.: SURVEY ON GRAPH NEURAL NETWORKS FOR TIME SERIES: FORECASTING, CLASSIFICATION, 10467\\nFig. 1. Graph neural networks for time series analysis (GNN4TS). In this\\nexample of a wind farm, different analytical tasks can be categorized into time\\nseries forecasting, classiﬁcation, anomaly detection, and imputation.\\nresults [23]. The advent of deep learning technologies has led',\n",
              " 'series forecasting, classiﬁcation, anomaly detection, and imputation.\\nresults [23]. The advent of deep learning technologies has led\\nto the development of different neural networks based on con-\\nvolutional neural networks (CNN) [24],[25], recurrent neural\\nnetworks (RNN) [26], and transformers [27], which have shown\\nsigniﬁcant advantages in modeling real-world time series data.\\nHowever, one of the biggest limitations of the above methods\\nis that they do not explicitly model the spatial relations existing\\nbetween time series in non-euclidean space [15], which limits\\ntheir expressiveness [28].\\nIn recent years, graph neural networks (GNNs) have emerged\\nas a powerful tool for learning non-euclidean data represen-\\ntations [29],[30],[31],[32], paving the way for modeling',\n",
              " 'In recent years, graph neural networks (GNNs) have emerged\\nas a powerful tool for learning non-euclidean data represen-\\ntations [29],[30],[31],[32], paving the way for modeling\\nreal-world time series data. This enables the capture of diverse\\nand intricate relationships, both inter-variable (connections be-\\ntween different variables within a multivariate series) and inter-\\ntemporal (dependencies between different points in time). Con-\\nsidering the complex spatial-temporal dependencies inherent in\\nreal-world scenarios, a line of studies has integrated GNNs with\\nvarious temporal modeling frameworks to capture both spatial\\nand temporal dynamics and demonstrate promising results [12],\\n[13],[14],[15],[16]. While early research efforts were primarily',\n",
              " 'various temporal modeling frameworks to capture both spatial\\nand temporal dynamics and demonstrate promising results [12],\\n[13],[14],[15],[16]. While early research efforts were primarily\\nconcentrated on various forecasting scenarios [13],[14],[15],\\nrecent advancements in time series analysis utilizing GNNs have\\ndemonstrated promising outcomes in other mainstream tasks.\\nThese include classiﬁcation [33],[34], anomaly detection [35],\\n[36], and imputation [37],[38].I nF i g . 1, we provide an overview\\nof graph neural networks for time series analysis (GNN4TS).\\nRelated Surveys. Despite the growing body of research per-\\nforming various time series analytic tasks with GNNs, existing\\nsurveys tend to focus on speciﬁc perspectives within a restricted',\n",
              " 'Related Surveys. Despite the growing body of research per-\\nforming various time series analytic tasks with GNNs, existing\\nsurveys tend to focus on speciﬁc perspectives within a restricted\\nscope. For instance, the survey by Wang et al. [11]offers a review\\nof deep learning techniques for spatial-temporal data mining, but\\nit does not speciﬁcally concentrate on GNN-based methods. The\\nsurvey by Ye et al. [12] zeroes in on graph-based deep learning\\narchitectures in the trafﬁc domain, primarily considering fore-\\ncasting scenarios. A recent survey by Jin et al. [15] offers anoverview of GNNs for predictive learning in urban computing,\\nbut neither extends its coverage to other application domains nor\\nthoroughly discusses other tasks related to time series analysis.',\n",
              " 'but neither extends its coverage to other application domains nor\\nthoroughly discusses other tasks related to time series analysis.\\nFinally, we mention the work by Rahmani et al. [17], which\\nexpands the survey of GNNs to many intelligent transportation\\nsystems, but tasks other than forecasting remain overlooked. A\\ndetailed comparison between our survey and others is presented\\nin Table I.\\nTo ﬁll the gap, this survey offers a comprehensive and up-to-\\ndate review of graph neural networks for time series analysis,\\nencompassing the majority of tasks ranging from time series\\nforecasting, classiﬁcation, anomaly detection, and imputation.\\nSpeciﬁcally, we ﬁrst provide two broad views to classify and\\ndiscuss existing works from the task- and methodology-oriented',\n",
              " 'forecasting, classiﬁcation, anomaly detection, and imputation.\\nSpeciﬁcally, we ﬁrst provide two broad views to classify and\\ndiscuss existing works from the task- and methodology-oriented\\nperspectives. Then, we delve into six popular application sectors\\nwithin the existing research of GNN4TS, and propose several\\npotential future research directions. The key contributions of our\\nsurvey are summarized as follows:rFirst Comprehensive Survey: To the best of our knowledge,\\nthis is the ﬁrst comprehensive survey that reviews the recent\\nadvances in mainstream time series analysis tasks with\\ngraph neural networks. It covers a wide range of recent\\nresearch and provides a broad view of the development of',\n",
              " 'advances in mainstream time series analysis tasks with\\ngraph neural networks. It covers a wide range of recent\\nresearch and provides a broad view of the development of\\nGNN4TS without restricting to speciﬁc tasks or domains.rUniﬁed and Structured Taxonomy: We present a uniﬁed\\nframework to structurally categorize existing works from\\ntask- and methodology-oriented perspectives. In the ﬁrst\\nclassiﬁcation, we offer an overview of tasks in time series\\nanalysis, covering different problem settings prevalent in\\nGNN-based research; and in the second classiﬁcation, we\\ndissect GNN4TS in terms of spatial and temporal depen-\\ndencies modeling and the overall model architecture.rDetailed and Current Overview: We conduct a compre-\\nhensive review that not only covers the breadth of the',\n",
              " 'dencies modeling and the overall model architecture.rDetailed and Current Overview: We conduct a compre-\\nhensive review that not only covers the breadth of the\\nﬁeld but also delves into the depth of individual studies\\nwith ﬁne-grained classiﬁcation and detailed discussion,\\nproviding readers with an up-to-date understanding of the\\nstate-of-the-art in GNN4TS.rBroadening Applications: We discuss the expanding appli-\\ncations of GNN4TS across various sectors, highlighting its\\nversatility and potential for future growth in diverse ﬁelds.rFuture Research Directions: We shed light on potential fu-\\nture research directions, offering insights and suggestions\\nthat could guide and inspire future research in the ﬁeld of\\nGNN4TS.\\nThe remainder of this survey is organized as follows:',\n",
              " 'ture research directions, offering insights and suggestions\\nthat could guide and inspire future research in the ﬁeld of\\nGNN4TS.\\nThe remainder of this survey is organized as follows:\\nSection IIprovides notations used throughout the paper.\\nSection IIIpresents the taxonomy of GNN4TS from different\\nperspectives. Sections IV,V,VI, and VIIreview the four major\\ntasks in the GNN4TS literature. Section VIII surveys popular\\napplications of GNN4TS across various ﬁelds, while Section IX\\nexamines open questions and potential future directions.\\nII. D EFINITION AND NOTATION\\nTime series data comprises a sequence of observations gath-\\nered or recorded over a period of time. This data can be either',\n",
              " 'II. D EFINITION AND NOTATION\\nTime series data comprises a sequence of observations gath-\\nered or recorded over a period of time. This data can be either\\nAuthorized licensed use limited to: Mapua University. Downloaded on April 02,2025 at 08:19:48 UTC from IEEE Xplore.  Restrictions apply. 10468 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 46, NO. 12, DECEMBER 2024\\nTABLE I\\nCOMPARISON BETWEEN OURSURVEY AND OTHER RELATED SURVEYS\\nregularly orirregularly sampled , with the latter also referred to\\nas time series data with missing values. Within each of these\\ncases, the data can be further classiﬁed into two primary types:\\nunivariate andmultivariate time series . In the sequel, we employ\\nbold uppercase letters (e.g., X), bold lowercase letters (e.g., x),',\n",
              " 'cases, the data can be further classiﬁed into two primary types:\\nunivariate andmultivariate time series . In the sequel, we employ\\nbold uppercase letters (e.g., X), bold lowercase letters (e.g., x),\\nand calligraphic letters (e.g., V) to denote matrices, vectors, and\\nsets, respectively.\\nDeﬁnition 1 (Univariate Time Series): A univariate time se-\\nries is a sequence of scalar observations collected over time,\\nwhich can be regularly or irregularly sampled. A regularly sam-\\npled univariate time series is deﬁned as X={x1,x2,...,x T}∈\\nRT, where xt∈R. For an irregularly sampled univariate\\ntime series, observations are collected at non-uniform time\\nintervals, such as X={(t1,x1),(t2,x2),...,(tT,xT)}∈RT,\\nwhere time points are non-uniformly spaced.',\n",
              " 'time series, observations are collected at non-uniform time\\nintervals, such as X={(t1,x1),(t2,x2),...,(tT,xT)}∈RT,\\nwhere time points are non-uniformly spaced.\\nDeﬁnition 2 (Multivariate Time Series): A multivariate time\\nseries is a sequence of N-dimensional vector observations col-\\nlected over time, i.e., X∈RN×T. A regularly sampled mul-\\ntivariate time series has vector observations collected at uni-\\nform time intervals, i.e., xt∈RN. In an irregularly sampled\\nmultivariate time series, there are possibly Nunaligned time\\nseries with respect to time steps, which implies only 0≤n≤N\\nobservations available at each time step.\\nThe majority of research based on GNNs focuses on modeling\\nmultivariate time series, as they can be naturally abstracted into',\n",
              " 'observations available at each time step.\\nThe majority of research based on GNNs focuses on modeling\\nmultivariate time series, as they can be naturally abstracted into\\nspatial-temporal graphs . This abstraction allows for an accurate\\ncharacterization of dynamic inter-temporal and inter-variable\\ndependencies. The former describes the relations between dif-\\nferent time steps within each time series (e.g., the temporal\\ndynamics of red nodes between t1andt3in Fig. 2), while the\\nlatter captures dependencies between time series (e.g., the spatial\\nrelations between four nodes at each time step in Fig. 2), such as\\nthe geographical information of the sensors generating the data\\nfor each variable. To illustrate this, we ﬁrst deﬁne attributed\\ngraphs .',\n",
              " 'the geographical information of the sensors generating the data\\nfor each variable. To illustrate this, we ﬁrst deﬁne attributed\\ngraphs .\\nDeﬁnition 3 (Attributed Graph): An attributed graph is a\\nstatic graph that associates each node with a set of attributes,\\nrepresenting node features. Formally, an attributed graph is de-\\nﬁned asG=(A,X), which consists of a (weighted) adjacency\\nmatrix A∈RN×Nand a node-feature matrix X∈RN×D.T h e\\nadjacency matrix represents the graph topology, which can beFig. 2. Examples of spatial-temporal graphs, where node colors represent\\ndistinct features. The top and bottom panels demonstrate spatio-temporal graphs\\nwith ﬁxed and dynamic graph structures over time, respectively.\\ncharacterized by V={v1,v2,...,v N},t h es e to f Nnodes, and',\n",
              " 'with ﬁxed and dynamic graph structures over time, respectively.\\ncharacterized by V={v1,v2,...,v N},t h es e to f Nnodes, and\\nE={eij:= (vi,vj)∈V×V| Aij/negationslash=0}, the set of edges; Aij\\nis the(i,j)-th entry in the adjacency matrix A. The feature ma-\\ntrixXcontains the node attributes, where the i-th row xi∈RD\\nrepresents the D-dimensional feature vector of node vi.\\nIn attributed graphs, multi-dimensional edge features can be\\nconsidered too, however, this paper assumes only scalar weights\\nencoded in the adjacency matrix to avoid overwhelming nota-\\ntions.\\nIn light of this, a spatial-temporal graph can be described as\\na series of attributed graphs, which effectively represent (multi-\\nvariate) time series data in conjunction with either evolving or\\nﬁxed structural information over time.',\n",
              " 'a series of attributed graphs, which effectively represent (multi-\\nvariate) time series data in conjunction with either evolving or\\nﬁxed structural information over time.\\nDeﬁnition 4 (Spatial-Temporal Graph): A spatial-temporal\\ngraph can be interpreted as a discrete-time dynamic graph [31],\\n[39], i.e.,G={G1,G2,...,GT}, whereGt=(At,Xt)denotes\\nan attributed graph at time t.At∈RN×NandXt∈RN×Dare\\ncorresponding adjacency and feature matrices. Atmay either\\nevolve over time or remain ﬁxed, depending on speciﬁc settings.\\nWhen abstracting time series data, we let Xt:=xt∈RN.\\nWe introduce graph neural networks as modern deep learning\\nmodels to process graph-structured data. The core operation in\\ntypical GNNs, often referred to as graph convolution ,i n v o l v e s',\n",
              " 'We introduce graph neural networks as modern deep learning\\nmodels to process graph-structured data. The core operation in\\ntypical GNNs, often referred to as graph convolution ,i n v o l v e s\\nexchanging information across neighboring nodes. In the context\\nof time series analysis, this operation enables us to explicitly\\nAuthorized licensed use limited to: Mapua University. Downloaded on April 02,2025 at 08:19:48 UTC from IEEE Xplore.  Restrictions apply. JIN et al.: SURVEY ON GRAPH NEURAL NETWORKS FOR TIME SERIES: FORECASTING, CLASSIFICATION, 10469\\nrely on the inter-variable dependencies represented by the graph\\nedges. Aware of the different nuances, we deﬁne GNNs in the\\nspatial domain, which involves transforming the input signal\\nwith learnable functions along the dimension of N.',\n",
              " 'edges. Aware of the different nuances, we deﬁne GNNs in the\\nspatial domain, which involves transforming the input signal\\nwith learnable functions along the dimension of N.\\nDeﬁnition 5 (Graph Neural Network): Given an attributed\\ngraphG=(A,X), we deﬁne xi=X[i,:]∈RDas theD-\\ndimensional feature vector of node vi. A GNN learns node rep-\\nresentations through two primary functions [40]:AGGREGATE (·)\\nand C OMBINE(·).T h eA GGREGATE (·)function computes and\\naggregates messages from neighboring nodes, while the\\nCOMBINE(·)function merges the aggregated and previous states\\nto transform node embeddings. Formally, the k-th layer in a\\nGNN is deﬁned by the extended\\na(k)\\ni=AGGREGATE(k)/parenleftBig/braceleftBig\\nh(k−1)\\nj:vj∈N(vi)/bracerightBig/parenrightBig\\n,\\nh(k)\\ni=COMBINE(k)/parenleftBig\\nh(k−1)\\ni,a(k)',\n",
              " 'GNN is deﬁned by the extended\\na(k)\\ni=AGGREGATE(k)/parenleftBig/braceleftBig\\nh(k−1)\\nj:vj∈N(vi)/bracerightBig/parenrightBig\\n,\\nh(k)\\ni=COMBINE(k)/parenleftBig\\nh(k−1)\\ni,a(k)\\ni/parenrightBig\\n, (1)\\nor, more generally, aggregating messages computed from both\\nsending and receiving nodes vjandvi, respectively. Here, a(k)\\ni\\nandh(k)\\nirepresent the aggregated message from neighbors and\\nthe transformed node embedding of node viin thek-th layer,\\nrespectively. The input and output of a GNN are h(0)\\ni:=xiand\\nh(K)\\ni:=hi.\\nThe above formulation in (1)is referred to as spatial GNNs ,\\nas opposed to spectral GNNs which deﬁnes convolution from\\nthe lens of spectral graph theory. We refer the reader to recent\\npublication [28] for a deeper analysis of spectral versus spatial',\n",
              " 'as opposed to spectral GNNs which deﬁnes convolution from\\nthe lens of spectral graph theory. We refer the reader to recent\\npublication [28] for a deeper analysis of spectral versus spatial\\nGNNs, and [29] for a comprehensive review of GNNs.\\nTo employ GNNs for time series analysis, it is implied that a\\ngraph structure must be provided. However, not all time series\\ndata have readily available graph structures and, in practice, two\\ntypes of strategies are utilized to generate the missing graph\\nstructures from the data: heuristics orlearned from data.\\nHeuristic-Based Graphs: This group of methods extracts\\ngraph structures from data based on heuristics, such as:rSpatial Proximity: This approach deﬁnes the graph struc-\\nture by considering the proximity between pairs of nodes',\n",
              " 'graph structures from data based on heuristics, such as:rSpatial Proximity: This approach deﬁnes the graph struc-\\nture by considering the proximity between pairs of nodes\\nbased on, e.g., their geographical location. A typical exam-\\nple is the construction of the adjacency matrix Abased on\\nthe shortest travel distance between nodes when the time\\nseries data have geospatial properties:\\nAi,j=/braceleftbigg1\\ndij,ifdij/negationslash=0,\\n0, otherwise ,(2)\\nwheredijdenotes the shortest travel distance between\\nnodeiand node j. Some common kernel functions, e.g.,\\nGaussian radial basis, can also be applied [15].rPairwise Connectivity: In this approach, the graph structure\\nis determined by the connectivity between pairs of nodes,\\nlike that determined by transportation networks. The adja-',\n",
              " 'is determined by the connectivity between pairs of nodes,\\nlike that determined by transportation networks. The adja-\\ncency matrix Ais deﬁned as:\\nAi,j=/braceleftbigg\\n1,ifviandvjare directly linked ,\\n0,otherwise .(3)\\nTypical scenarios include edges representing roads, rail-\\nways, or adjacent regions [41],[42]. In such cases, thegraph can be undirected or directed, resulting in symmetric\\nand asymmetric adjacency matrices.rPairwise Similarity: This method constructs the graph by\\nconnecting nodes with similar attributes. A simple example\\nis the construction of adjacency matrix Abased on the\\ncosine similarity between time series:\\nAi,j=x/latticetop\\nixj\\n/bardblxi/bardbl/bardblxj/bardbl, (4)\\nwhere/bardbl·/bardbl denotes the euclidean norm. There are also',\n",
              " 'cosine similarity between time series:\\nAi,j=x/latticetop\\nixj\\n/bardblxi/bardbl/bardblxj/bardbl, (4)\\nwhere/bardbl·/bardbl denotes the euclidean norm. There are also\\nseveral variants for creating similarity-based graphs, such\\nas Pearson correlation coefﬁcient (PCC) [43] and dynamic\\ntime warping (DTW) [44].rFunctional Dependence: This approach deﬁnes the graph\\nstructure based on known functional dependencies be-\\ntween pairs of nodes, such as direct causal relationships\\nor dependence from common hidden factors. For instance,\\nadjacency matrix Acan be constructed based on Granger\\ncausality [45] as\\nAi,j=⎧\\n⎨\\n⎩1,if nodejGranger-causes\\nnodeiat a signiﬁcance level α,\\n0,otherwise .(5)\\nOther examples involve transfer entropy (TE) [46] and\\ndirected phase lag index (DPLI) [47]. While overlap with',\n",
              " 'Ai,j=⎧\\n⎨\\n⎩1,if nodejGranger-causes\\nnodeiat a signiﬁcance level α,\\n0,otherwise .(5)\\nOther examples involve transfer entropy (TE) [46] and\\ndirected phase lag index (DPLI) [47]. While overlap with\\nprevious heuristics exists, functional relations typically\\nrepresent or estimate actual node dependencies in the data-\\ngenerating process.\\nLearning-Based Graphs: In contrast to heuristic-based meth-\\nods, learning-based approaches aim to learn the graph structure\\ndirectly from the data and end-to-end with the downstream task.\\nAccordingly, graph Acan be deﬁned as a function ρ(·)of some\\ntrainable model parameters Θand, possibly, also of the time\\nseries observations X, i.e.,A=ρ(Θ,X). Embedding-based\\napproaches (e.g., [48],[49]) deﬁne the presence of each edge by',\n",
              " 'trainable model parameters Θand, possibly, also of the time\\nseries observations X, i.e.,A=ρ(Θ,X). Embedding-based\\napproaches (e.g., [48],[49]) deﬁne the presence of each edge by\\ncomparing learned embedding vectors of the associated nodes;\\nas an example, consider Ai,j=ReLU(Θ/latticetop\\niΘj)withΘi,Θj\\nnode embedding of nodes i,j, respectively. Another common\\nexample involves deﬁning Athrough attention scores computed\\nbetween node signals [50]. Sparsiﬁcation methods – such as the\\nReLU activation above or top-k selection – are often applied\\nto discard edges with null or low weights, thereby reducing the\\ncomputational load requested by dense GNN operations [51].A n\\nalternative strategy is modeling Aas a discrete random variable\\nwhose parametric distribution, say pΘ(A|X), is learned along-',\n",
              " 'computational load requested by dense GNN operations [51].A n\\nalternative strategy is modeling Aas a discrete random variable\\nwhose parametric distribution, say pΘ(A|X), is learned along-\\nside the other model parameters [52],[53]. Learning-based ap-\\nproaches enable the data-driven discovery of less obvious graph\\nstructures that are tailored to solve the given task, potentially\\nproviding more effective relations than heuristic-based graphs.\\nIII. F RAMEWORK AND CATEGORIZATION\\nIn this section, we present a comprehensive task-oriented\\ntaxonomy for GNNs within the context of time series analysis\\n(Section III-A ). Subsequently, we investigate how to encode time\\nseries across various tasks by introducing a uniﬁed methodologi-\\ncal framework for GNN architectures (Section III-B ). According',\n",
              " '(Section III-A ). Subsequently, we investigate how to encode time\\nseries across various tasks by introducing a uniﬁed methodologi-\\ncal framework for GNN architectures (Section III-B ). According\\nAuthorized licensed use limited to: Mapua University. Downloaded on April 02,2025 at 08:19:48 UTC from IEEE Xplore.  Restrictions apply. 10470 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 46, NO. 12, DECEMBER 2024\\nFig. 3. Task-oriented taxonomy of graph neural networks for time series analysis in the existing literature.\\nto the framework, all architectures are composed of a similar\\ngraph-based processing module fθand a second module pφ\\nspecialized in downstream tasks.\\nA. Task-Oriented Taxonomy\\nIn Fig. 3, we illustrate a task-oriented taxonomy of GNNs',\n",
              " 'graph-based processing module fθand a second module pφ\\nspecialized in downstream tasks.\\nA. Task-Oriented Taxonomy\\nIn Fig. 3, we illustrate a task-oriented taxonomy of GNNs\\nencompassing the primary tasks and mainstream modeling for\\ntime series analysis, and showcasing the potential of GNN4TS.\\nThis survey focuses on four categories: time series forecasting ,\\nanomaly detection ,imputation , and classiﬁcation . These tasks\\nare performed on top of the time series representations learned\\nbyspatial-temporal graph neural networks (STGNNs), which\\nserve as the foundation for encoding time series data in existing\\nliterature across various tasks. We detail this in Section III-B .\\nTime Series Forecasting: This task is centered around pre-\\ndicting future values of the time series based on historical',\n",
              " 'literature across various tasks. We detail this in Section III-B .\\nTime Series Forecasting: This task is centered around pre-\\ndicting future values of the time series based on historical\\nobservations, as depicted in Fig. 4(a). Depending on application\\nneeds, we categorize this task into two types: single-step-ahead\\nforecasting and multi-step-ahead forecasting . The former is\\nmeant to predict single future observations of the time series\\nonce at a time, i.e., the target at time tisY:=Xt+Hfor some\\nH∈Nsteps ahead, while the latter makes predictions for a time\\ninterval, e.g., Y:=Xt+1:t+H. Parameterized solutions to bothpredictive cases can be derived by optimizing\\nθ∗,φ∗=a r gm i n\\nθ,φLF(pφ(fθ(Xt−T:t,At−T:t)),Y), (6)\\nwherefθ(·)andpφ(·)represent a spatial-temporal GNN and the',\n",
              " 'θ∗,φ∗=a r gm i n\\nθ,φLF(pφ(fθ(Xt−T:t,At−T:t)),Y), (6)\\nwherefθ(·)andpφ(·)represent a spatial-temporal GNN and the\\npredictor, respectively. Details regarding the fθ(·)architecture\\nare given in Section III-B while the predictor is, normally, a\\nmulti-layer perceptron. In the sequel, we denote by Xt−T:tand\\nAt−T:ta spatial-temporal graph G={Gt−T,Gt−T+1,...,Gt}\\nwith length T. If the underlying graph structure is ﬁxed, then\\nAt:=A.LF(·)denotes the forecasting loss, which is typi-\\ncally a squared or absolute loss function, e.g., STGCN [54]\\nand MTGNN [51]. Most existing works minimize the error\\nbetween the forecasting and the ground truth Ythrough (6);\\nthis process is known as deterministic time series forecasting.\\nBesides, we have probabilistic time series forecasting methods,',\n",
              " 'between the forecasting and the ground truth Ythrough (6);\\nthis process is known as deterministic time series forecasting.\\nBesides, we have probabilistic time series forecasting methods,\\nsuch as DiffSTG [55], that share the same objective (6)function\\nthough it is not directly optimized. Based on the size of the fore-\\ncasting horizon H, we end up in either short-term orlong-term\\nforecasting .\\nTime Series Anomaly Detection: This task focuses on de-\\ntecting irregularities and unexpected events in time series data\\n(Fig. 4(b)). Detecting anomalies requires determining when\\nthe anomalous event occurred, while diagnosing them requests\\ngaining insights about how and why the anomaly occurred. Due',\n",
              " '(Fig. 4(b)). Detecting anomalies requires determining when\\nthe anomalous event occurred, while diagnosing them requests\\ngaining insights about how and why the anomaly occurred. Due\\nAuthorized licensed use limited to: Mapua University. Downloaded on April 02,2025 at 08:19:48 UTC from IEEE Xplore.  Restrictions apply. JIN et al.: SURVEY ON GRAPH NEURAL NETWORKS FOR TIME SERIES: FORECASTING, CLASSIFICATION, 10471\\nFig. 4. Four categories of graph neural networks for time series analysis. For the sake of simplicity and illustrative purposes, we assume the graph st ructures are\\nﬁxed in all subplots.\\nto the general difﬁculty of acquiring anomaly events, current\\nresearch commonly treats anomaly detection as an unsuper-\\nvised problem that involves the design of a model describing',\n",
              " 'to the general difﬁculty of acquiring anomaly events, current\\nresearch commonly treats anomaly detection as an unsuper-\\nvised problem that involves the design of a model describing\\nnormal, non-anomalous data. The learned model is then used\\nto detect anomalies by generating a high score whenever an\\nanomaly event occurs. This model learning process mirrors the\\nforecasting optimization, (6), withfθ(·)andpφ(·)denote the\\nspatial-temporal GNN and the predictor, respectively. In general,\\nthe spatial-temporal GNN and the predictor are trained on nor-\\nmal, non-anomalous data using either forecasting [36],[56] or\\nreconstruction [35],[57] optimization approaches, with the aim\\nof minimizing the discrepancy between the normal input and the',\n",
              " 'mal, non-anomalous data using either forecasting [36],[56] or\\nreconstruction [35],[57] optimization approaches, with the aim\\nof minimizing the discrepancy between the normal input and the\\nforecast (or reconstructed) series. Nonetheless, when these mod-\\nels are put to use for detecting anomalies, they are expected to fail\\nin minimizing this discrepancy upon receiving anomalous input.\\nThis inability to conform to the expected low-discrepancy model\\nbehavior during anomaly periods creates a detectable difference,\\nfacilitating the detection of anomalies. The threshold separating\\nnormal and anomalous data is a sensitive hyperparameter that\\nshould be set considering the rarity of anomalies and aligned\\nwith a desired false alarm rate [58]. Lastly, to diagnose the',\n",
              " 'normal and anomalous data is a sensitive hyperparameter that\\nshould be set considering the rarity of anomalies and aligned\\nwith a desired false alarm rate [58]. Lastly, to diagnose the\\ncauses of anomalies, a common strategy involves calculatingdiscrepancies for each channel node and consolidating these\\ninto a single anomaly score [59]. This approach allows for\\nthe identiﬁcation of the channel variables responsible for the\\nanomaly events by calculating their respective contributions to\\nthe ﬁnal score.\\nTime Series Imputation: This task is centered around estimat-\\ning and ﬁlling in missing or incomplete data points within a time\\nseries (Fig. 4(c)). Current research in this domain can be broadly\\nclassiﬁed into two main approaches: in-sample imputation and',\n",
              " 'ing and ﬁlling in missing or incomplete data points within a time\\nseries (Fig. 4(c)). Current research in this domain can be broadly\\nclassiﬁed into two main approaches: in-sample imputation and\\nout-of-sample imputation . In-sample imputation involves ﬁlling\\nmissing values in a given time series, while out-of-sample im-\\nputation pertains to inferring missing data not present in the\\ntraining dataset. We formulate the learning objective as follows:\\nθ∗,φ∗=a r gm i n\\nθ,φLI/parenleftBig\\npφ/parenleftBig\\nfθ(˜Xt−T:t,At−T:t)/parenrightBig\\n,Xt−T:t/parenrightBig\\n,\\n(7)\\nwherefθ(·)andpφ(·)denote the spatial-temporal GNN and\\nimputation module to be learned, respectively. The imputation\\nmodule can e.g., be a multi-layer perceptron. In this task, ˜Xt−T:t',\n",
              " ',\\n(7)\\nwherefθ(·)andpφ(·)denote the spatial-temporal GNN and\\nimputation module to be learned, respectively. The imputation\\nmodule can e.g., be a multi-layer perceptron. In this task, ˜Xt−T:t\\nrepresents input time series data with missing values (reference\\ntime series), while Xt−T:tdenotes the same time series without\\nAuthorized licensed use limited to: Mapua University. Downloaded on April 02,2025 at 08:19:48 UTC from IEEE Xplore.  Restrictions apply. 10472 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 46, NO. 12, DECEMBER 2024\\nmissing values. As it is impossible to access the reference\\ntime series during training, a surrogate optimization objective is\\nconsidered, such as generating synthetic missing values [37].I n',\n",
              " 'missing values. As it is impossible to access the reference\\ntime series during training, a surrogate optimization objective is\\nconsidered, such as generating synthetic missing values [37].I n\\n(7),LI(·)refers to the imputation loss, which can be, for instance,\\nan absolute or a squared error, similar to forecasting tasks. For in-\\nsample imputation, the model is trained and evaluated on ˜Xt−T:t\\nandXt−T:t. Instead, for out-of-sample imputation, the model\\nis trained and evaluated on disjoint sequences, e.g., trained on\\n˜Xt−T:tbut evaluated on Xt:t+H, where the missing values in\\n˜Xt:t+Hwill be estimated. Similar to time series forecasting\\nand anomaly detection, the imputation process can be either\\ndeterministic orprobabilistic . The former predicts the missing',\n",
              " '˜Xt:t+Hwill be estimated. Similar to time series forecasting\\nand anomaly detection, the imputation process can be either\\ndeterministic orprobabilistic . The former predicts the missing\\nvalues directly (e.g., GRIN [37]), while the latter estimates the\\nmissing values from data distributions (e.g., PriSTI [38]).\\nTime Series Classiﬁcation: This task aims to assign a cat-\\negorical label to a given time series based on its underlying\\npatterns or characteristics. Rather than capturing patterns within\\na time series data sample, the essence of time series classiﬁcation\\nresides in discerning differentiating patterns that help separate\\nsamples based on their class labels. The optimization problem\\ncan be expressed as:\\nθ∗,φ∗=a r gm i n\\nθ,φLC(pφ(fθ(X,A)),Y), (8)']"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "texts[:50]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAreGWLPT3X6"
      },
      "source": [
        "## Load dataset to vector store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1tePf54T5Iv",
        "outputId": "7681ba4a-b087-4bce-a1ac-9f2f3b3e07c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inserted 205 headlines\n"
          ]
        }
      ],
      "source": [
        "astra_vector_store.add_texts(texts)\n",
        "\n",
        "print(f\"Inserted {len(texts)} headlines\")\n",
        "\n",
        "astra_vector_index = VectorStoreIndexWrapper(vectorstore=astra_vector_store)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHU0yU7QUYJt"
      },
      "source": [
        "## Run QA Cycle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1P7QkZ4UZ7x",
        "outputId": "898a6217-9c0d-4b13-de91-5b26b216fe9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: What are GNNs\n",
            "Answer: GNNs are graph neural networks, which are modern deep learning models used to process graph-structured data. They involve exchanging information across neighboring nodes and rely on the inter-variable dependencies represented by the graph edges. GNNs are defined in the spatial domain and involve transforming the input signal with learnable functions along the dimension of N.\n",
            "First documents by relevance:\n",
            "0.9215013677038083 \"We introduce graph neural networks as modern deep learning\n",
            "models to process graph-s...\"\n",
            "0.9214874918418028 \"edges. Aware of the different nuances, we deﬁne GNNs in the\n",
            "spatial domain, which in...\"\n",
            "0.9214850672915897 \"We introduce graph neural networks as modern deep learning\n",
            "models to process graph-s...\"\n",
            "0.921469936265626 \"edges. Aware of the different nuances, we deﬁne GNNs in the\n",
            "spatial domain, which in...\"\n",
            "Question: What are the limitations of GNNs\n",
            "Answer: The limitations of GNNs include their lack of explicit modeling of spatial relations between time series in non-euclidean space, which limits their expressiveness.\n",
            "First documents by relevance:\n",
            "0.9267737004619745 \"Related Surveys. Despite the growing body of research per-\n",
            "forming various time seri...\"\n",
            "0.926762172182992 \"Related Surveys. Despite the growing body of research per-\n",
            "forming various time seri...\"\n",
            "0.9231132195902902 \"series forecasting, classiﬁcation, anomaly detection, and imputation.\n",
            "results [23]. ...\"\n",
            "0.923078316010625 \"series forecasting, classiﬁcation, anomaly detection, and imputation.\n",
            "results [23]. ...\"\n"
          ]
        }
      ],
      "source": [
        "first_question = True\n",
        "while True:\n",
        "  if first_question:\n",
        "    query_text = input(\"Ask a question or type 'quit' to exit: \").strip()\n",
        "  else:\n",
        "    query_text = input(\"What is your next question or type 'quit' to exit: \").strip()\n",
        "  if query_text.lower() == 'quit':\n",
        "    break\n",
        "  if query_text == '':\n",
        "    continue\n",
        "\n",
        "  first_question = False\n",
        "  print(f\"Question: {query_text}\")\n",
        "  response = astra_vector_index.query(query_text, llm=llm).strip()\n",
        "  print(f\"Answer: {response}\")\n",
        "\n",
        "  print(\"First documents by relevance:\")\n",
        "  for doc, score in astra_vector_store.similarity_search_with_score(query_text, k=4):\n",
        "    print(f\"{score} \\\"{doc.page_content[:84]}...\\\"\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
